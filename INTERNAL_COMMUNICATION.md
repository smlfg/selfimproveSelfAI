# SelfAI Internal Communication & Data Flow

VollstÃ¤ndige technische Dokumentation der internen Kommunikationsstruktur, DatenflÃ¼sse und Komponenten-Interaktionen.

---

## Table of Contents

1. [Startup & Initialization](#1-startup--initialization)
2. [Normal Chat Flow](#2-normal-chat-flow-ohne-plan)
3. [DPPM Pipeline Flow](#3-dppm-pipeline-flow-mit-plan)
4. [Tool Execution (Smolagents)](#4-tool-execution-smolagents)
5. [External Process Communication](#5-external-process-communication)
6. [Memory System](#6-memory-system)
7. [Context Loading](#7-context-loading)
8. [Parallel Execution](#8-parallel-execution)
9. [Error Handling & Retry](#9-error-handling--retry)
10. [Data Structures](#10-data-structures)

---

## 1. Startup & Initialization

### Entry Point: `main()` in `selfai/selfai.py:1077`

```
python selfai/selfai.py
    â†“
main() function executes
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Initialize UI                               â”‚
â”‚ ui = TerminalUI()                                   â”‚
â”‚ - Singleton for all output operations               â”‚
â”‚ - Methods: status(), stream_prefix(), typing_anim() â”‚
â”‚ - NOT thread-safe (but output is serialized)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Initialize AgentManager                     â”‚
â”‚ agent_manager = AgentManager(agents_dir)            â”‚
â”‚                                                     â”‚
â”‚ Scans: agents/ directory                            â”‚
â”‚ For each subdirectory:                              â”‚
â”‚   - Reads system_prompt.md â†’ agent.system_prompt    â”‚
â”‚   - Reads memory_categories.txt â†’ List[str]         â”‚
â”‚   - Reads workspace_slug.txt â†’ str                  â”‚
â”‚   - Reads description.txt â†’ str                     â”‚
â”‚                                                     â”‚
â”‚ Creates: List[Agent] objects                        â”‚
â”‚ Storage: In-memory list                             â”‚
â”‚ Active: agent_manager.active_agent (Agent | None)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Initialize MemorySystem                     â”‚
â”‚ memory_system = MemorySystem(memory_dir)            â”‚
â”‚                                                     â”‚
â”‚ Structure: memory/                                  â”‚
â”‚   â”œâ”€â”€ {category}/                                   â”‚
â”‚   â”‚   â””â”€â”€ {agent}_{timestamp}.txt                   â”‚
â”‚   â””â”€â”€ plans/                                        â”‚
â”‚       â””â”€â”€ {timestamp}_{goal}.json                   â”‚
â”‚                                                     â”‚
â”‚ Methods:                                            â”‚
â”‚   - save_conversation() â†’ writes .txt file          â”‚
â”‚   - load_relevant_context() â†’ reads + filters       â”‚
â”‚   - save_plan() â†’ writes .json file                 â”‚
â”‚                                                     â”‚
â”‚ Storage: Filesystem (NO database!)                  â”‚
â”‚                                                     â”‚
â”‚ NEW: Context Window                                 â”‚
â”‚   - session_start = datetime.now()                  â”‚
â”‚   - context_window_minutes = 30 (default)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Initialize TokenLimits                      â”‚
â”‚ token_limits = TokenLimits()                        â”‚
â”‚ token_limits.set_balanced()  # Default profile      â”‚
â”‚                                                     â”‚
â”‚ Fields (all int):                                   â”‚
â”‚   - planner_max_tokens: 768                         â”‚
â”‚   - execution_max_tokens: 512                       â”‚
â”‚   - merge_max_tokens: 2048                          â”‚
â”‚   - tool_creation_max_tokens: 1024                  â”‚
â”‚   - error_correction_max_tokens: 1024               â”‚
â”‚   - selfimprove_max_tokens: 2048                    â”‚
â”‚   - chat_max_tokens: 1024                           â”‚
â”‚                                                     â”‚
â”‚ Storage: In-memory (runtime only)                   â”‚
â”‚ Control: /tokens, /extreme commands                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: Load LLM Backends                           â”‚
â”‚ execution_backends: List[Dict]                      â”‚
â”‚                                                     â”‚
â”‚ Priority Order:                                     â”‚
â”‚ 1. MiniMax (Cloud API)                              â”‚
â”‚    if config.minimax:                               â”‚
â”‚      interface = MinimaxInterface(...)              â”‚
â”‚      execution_backends.append({                    â”‚
â”‚        "interface": interface,                      â”‚
â”‚        "label": "MiniMax",                          â”‚
â”‚        "name": "minimax",                           â”‚
â”‚        "type": "cloud"                              â”‚
â”‚      })                                             â”‚
â”‚                                                     â”‚
â”‚ 2. CPU Fallback (llama-cpp-python)                  â”‚
â”‚    if config.cpu_fallback:                          â”‚
â”‚      interface = LocalLLMInterface(...)             â”‚
â”‚      execution_backends.append({                    â”‚
â”‚        "interface": interface,                      â”‚
â”‚        "label": "CPU Fallback",                     â”‚
â”‚        "name": "cpu",                               â”‚
â”‚        "type": "local"                              â”‚
â”‚      })                                             â”‚
â”‚                                                     â”‚
â”‚ Result: At least 1 backend must be available        â”‚
â”‚ Fallback Chain: MiniMax â†’ CPU                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 6: Load Planner Providers (Optional)           â”‚
â”‚ planner_providers: Dict[str, Dict]                  â”‚
â”‚                                                     â”‚
â”‚ Example:                                            â”‚
â”‚ {                                                   â”‚
â”‚   "minimax-planner": {                              â”‚
â”‚     "interface": PlannerMinimaxInterface(...),      â”‚
â”‚     "type": "minimax",                              â”‚
â”‚     "model": "abab6.5s-chat",                       â”‚
â”‚     "max_tokens": 768,                              â”‚
â”‚     "base_url": "...",                              â”‚
â”‚     "timeout": 180.0                                â”‚
â”‚   }                                                 â”‚
â”‚ }                                                   â”‚
â”‚                                                     â”‚
â”‚ Active: active_planner_provider (str | None)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 7: Load Merge Providers (Optional)             â”‚
â”‚ merge_providers: Dict[str, Dict]                    â”‚
â”‚ Similar structure to planner_providers              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 8: Enter Main Loop                             â”‚
â”‚ while True:                                         â”‚
â”‚   user_input = input("\nDu: ")                      â”‚
â”‚   # Process commands or chat                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Normal Chat Flow (OHNE /plan)

### User Input: "ErklÃ¤re mir Python"

```
User types: "ErklÃ¤re mir Python"
    â†“
input() returns string
    â†“
Check for commands (/, quit)
    â†“ No command â†’ Normal Chat
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Context from Memory                            â”‚
â”‚ history = memory_system.load_relevant_context(      â”‚
â”‚     agent=active_agent,                             â”‚
â”‚     context_hint=user_input,                        â”‚
â”‚     limit=5                                         â”‚
â”‚ )                                                   â”‚
â”‚                                                     â”‚
â”‚ Internal Process:                                   â”‚
â”‚ 1. Get candidate files from agent.memory_categories â”‚
â”‚ 2. Filter by time: mtime >= cutoff (NEW!)          â”‚
â”‚ 3. Extract tags from current input                  â”‚
â”‚ 4. Score each file by tag overlap                   â”‚
â”‚ 5. Filter by threshold (0.35)                       â”‚
â”‚ 6. Sort by (score, mtime) DESC                      â”‚
â”‚ 7. Take top N, re-sort chronologically              â”‚
â”‚                                                     â”‚
â”‚ Returns: List[Dict[str, str]]                       â”‚
â”‚ [                                                   â”‚
â”‚   {"role": "user", "content": "Previous Q"},        â”‚
â”‚   {"role": "assistant", "content": "Previous A"},   â”‚
â”‚   ...                                               â”‚
â”‚ ]                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Select Backend                                      â”‚
â”‚ backend = execution_backends[active_index]          â”‚
â”‚ interface = backend["interface"]  # MinimaxInterfaceâ”‚
â”‚ label = backend["label"]  # "MiniMax"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Check Streaming Support                             â”‚
â”‚ use_stream = hasattr(interface, "stream_generate")  â”‚
â”‚                                                     â”‚
â”‚ If streaming:                                       â”‚
â”‚   for chunk in interface.stream_generate_response():â”‚
â”‚     ui.streaming_chunk(chunk)                       â”‚
â”‚   response = "".join(chunks)                        â”‚
â”‚                                                     â”‚
â”‚ Else (blocking):                                    â”‚
â”‚   response = interface.generate_response(...)       â”‚
â”‚   ui.typing_animation(response)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Interface (MinimaxInterface)                    â”‚
â”‚                                                     â”‚
â”‚ Build Request:                                      â”‚
â”‚ {                                                   â”‚
â”‚   "model": "abab6.5s-chat",                         â”‚
â”‚   "messages": [                                     â”‚
â”‚     {                                               â”‚
â”‚       "role": "system",                             â”‚
â”‚       "content": agent.system_prompt                â”‚
â”‚     },                                              â”‚
â”‚     ...history messages...,                         â”‚
â”‚     {                                               â”‚
â”‚       "role": "user",                               â”‚
â”‚       "content": user_input                         â”‚
â”‚     }                                               â”‚
â”‚   ],                                                â”‚
â”‚   "max_tokens": token_limits.chat_max_tokens,      â”‚
â”‚   "temperature": 0.7,                               â”‚
â”‚   "stream": true                                    â”‚
â”‚ }                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HTTP POST to MiniMax API                            â”‚
â”‚ POST https://api.minimax.chat/v1/text/chatcompletionâ”‚
â”‚ Headers:                                            â”‚
â”‚   Authorization: Bearer {api_key}                   â”‚
â”‚   Content-Type: application/json                    â”‚
â”‚                                                     â”‚
â”‚ Response (SSE Stream):                              â”‚
â”‚ data: {"choices":[{"delta":{"content":"Python"}}]}  â”‚
â”‚ data: {"choices":[{"delta":{"content":" ist"}}]}    â”‚
â”‚ data: {"choices":[{"delta":{"content":"..."}}]}     â”‚
â”‚ data: [DONE]                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stream Processing                                   â”‚
â”‚ for line in response_stream:                        â”‚
â”‚   if line.startswith("data: "):                     â”‚
â”‚     json_str = line[6:]  # Remove "data: "          â”‚
â”‚     if json_str == "[DONE]":                        â”‚
â”‚       break                                         â”‚
â”‚     data = json.loads(json_str)                     â”‚
â”‚     chunk = data["choices"][0]["delta"]["content"]  â”‚
â”‚     yield chunk  # Generator pattern                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Display Output                                      â”‚
â”‚ ui.stream_prefix("MiniMax")  # Shows "MiniMax: "    â”‚
â”‚ ui.streaming_chunk("Python")                        â”‚
â”‚ ui.streaming_chunk(" ist")                          â”‚
â”‚ ui.streaming_chunk("...")                           â”‚
â”‚ print()  # Newline after stream complete            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save to Memory                                      â”‚
â”‚ memory_system.save_conversation(                    â”‚
â”‚     agent=active_agent,                             â”‚
â”‚     user_message=user_input,                        â”‚
â”‚     assistant_message=response                      â”‚
â”‚ )                                                   â”‚
â”‚                                                     â”‚
â”‚ Internal Process:                                   â”‚
â”‚ 1. Determine category (agent.memory_categories[0])  â”‚
â”‚ 2. Create category dir: memory/{category}/          â”‚
â”‚ 3. Generate filename: {slug}_{timestamp}.txt        â”‚
â”‚ 4. Extract tags from conversation                   â”‚
â”‚ 5. Format content:                                  â”‚
â”‚    ---                                              â”‚
â”‚    Agent: {display_name}                            â”‚
â”‚    AgentKey: {key}                                  â”‚
â”‚    Timestamp: {iso_datetime}                        â”‚
â”‚    Tags: {tags}                                     â”‚
â”‚    ---                                              â”‚
â”‚    System Prompt:                                   â”‚
â”‚    {system_prompt}                                  â”‚
â”‚    ---                                              â”‚
â”‚    User:                                            â”‚
â”‚    {user_message}                                   â”‚
â”‚    ---                                              â”‚
â”‚    SelfAI:                                          â”‚
â”‚    {assistant_message}                              â”‚
â”‚ 6. Write to file                                    â”‚
â”‚                                                     â”‚
â”‚ Result: Path to saved file                          â”‚
â”‚ Example: memory/code_helfer/main_20250118-142305.txtâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Back to Main Loop (while True)
```

### Data Flow Diagram:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚ "ErklÃ¤re Python"
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Main Loop  â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â”œâ”€â†’ MemorySystem.load_relevant_context()
      â”‚   â””â”€â†’ Returns: history (List[Dict])
      â”‚
      â”œâ”€â†’ Backend.generate_response()
      â”‚   â”œâ”€â†’ HTTP POST to MiniMax API
      â”‚   â”œâ”€â†’ SSE Stream Response
      â”‚   â””â”€â†’ Returns: response (str)
      â”‚
      â”œâ”€â†’ UI.display()
      â”‚   â””â”€â†’ Terminal Output
      â”‚
      â””â”€â†’ MemorySystem.save_conversation()
          â””â”€â†’ Filesystem Write
```

---

## 3. DPPM Pipeline Flow (MIT /plan)

### User Input: "/plan Build REST API"

```
User types: "/plan Build REST API"
    â†“
Detect command: startswith("/plan")
    â†“
Extract goal: "Build REST API"
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: PLANNING                                   â”‚
â”‚ Goal: Decompose into subtasks                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Build PlannerContext                                â”‚
â”‚ context = PlannerContext(                           â”‚
â”‚     available_agents=[                              â”‚
â”‚         {"key": "code_helfer", "description": "..."} â”‚
â”‚         {"key": "architect", "description": "..."}   â”‚
â”‚     ],                                              â”‚
â”‚     available_engines=["minimax", "smolagent"],     â”‚
â”‚     memory_summary="15 recent conversations",       â”‚
â”‚     system_info={                                   â”‚
â”‚         "os": "linux",                              â”‚
â”‚         "ram_gb": 16,                               â”‚
â”‚         "cpu_cores": 8                              â”‚
â”‚     }                                               â”‚
â”‚ )                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Call Planner                                        â”‚
â”‚ plan_data = planner_interface.plan(                 â”‚
â”‚     goal=goal_text,                                 â”‚
â”‚     context=planner_context,                        â”‚
â”‚     progress_callback=ui.streaming_chunk            â”‚
â”‚ )                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PlannerMinimaxInterface.plan()                      â”‚
â”‚                                                     â”‚
â”‚ Step 1: Build Prompt Template                       â”‚
â”‚ """                                                 â”‚
â”‚ You are a DPPM task planner.                        â”‚
â”‚                                                     â”‚
â”‚ GOAL: {goal}                                        â”‚
â”‚                                                     â”‚
â”‚ AVAILABLE AGENTS:                                   â”‚
â”‚ - code_helfer: Coding assistant                     â”‚
â”‚ - architect: System design                          â”‚
â”‚                                                     â”‚
â”‚ AVAILABLE ENGINES:                                  â”‚
â”‚ - minimax: LLM text generation                      â”‚
â”‚ - smolagent: Tool-calling agent                     â”‚
â”‚                                                     â”‚
â”‚ CONTEXT:                                            â”‚
â”‚ {memory_summary}                                    â”‚
â”‚ {system_info}                                       â”‚
â”‚                                                     â”‚
â”‚ Generate JSON plan:                                 â”‚
â”‚ {                                                   â”‚
â”‚   "subtasks": [                                     â”‚
â”‚     {                                               â”‚
â”‚       "id": "S1",                                   â”‚
â”‚       "title": "...",                               â”‚
â”‚       "objective": "...",                           â”‚
â”‚       "agent_key": "architect",                     â”‚
â”‚       "engine": "minimax",                          â”‚
â”‚       "parallel_group": 1,                          â”‚
â”‚       "depends_on": []                              â”‚
â”‚     }                                               â”‚
â”‚   ],                                                â”‚
â”‚   "merge": {                                        â”‚
â”‚     "strategy": "..."                               â”‚
â”‚   }                                                 â”‚
â”‚ }                                                   â”‚
â”‚ """                                                 â”‚
â”‚                                                     â”‚
â”‚ Step 2: HTTP POST to MiniMax                        â”‚
â”‚ max_tokens: token_limits.planner_max_tokens (768)   â”‚
â”‚                                                     â”‚
â”‚ Step 3: Parse JSON Response                         â”‚
â”‚ response_text = llm_response                        â”‚
â”‚ # Extract JSON (remove markdown if present)         â”‚
â”‚ plan_data = json.loads(response_text)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Returns Plan (Example)                          â”‚
â”‚ {                                                   â”‚
â”‚   "subtasks": [                                     â”‚
â”‚     {                                               â”‚
â”‚       "id": "S1",                                   â”‚
â”‚       "title": "Design API Schema",                 â”‚
â”‚       "objective": "Define REST endpoints...",      â”‚
â”‚       "agent_key": "architect",                     â”‚
â”‚       "engine": "minimax",                          â”‚
â”‚       "parallel_group": 1,                          â”‚
â”‚       "depends_on": []                              â”‚
â”‚     },                                              â”‚
â”‚     {                                               â”‚
â”‚       "id": "S2",                                   â”‚
â”‚       "title": "Implement Routes",                  â”‚
â”‚       "objective": "Code Flask routes...",          â”‚
â”‚       "agent_key": "code_helfer",                   â”‚
â”‚       "engine": "smolagent",                        â”‚
â”‚       "parallel_group": 2,                          â”‚
â”‚       "depends_on": ["S1"],                         â”‚
â”‚       "tools": ["run_aider_task"]                   â”‚
â”‚     },                                              â”‚
â”‚     {                                               â”‚
â”‚       "id": "S3",                                   â”‚
â”‚       "title": "Write Tests",                       â”‚
â”‚       "objective": "Create pytest tests...",        â”‚
â”‚       "agent_key": "code_helfer",                   â”‚
â”‚       "engine": "minimax",                          â”‚
â”‚       "parallel_group": 2,                          â”‚
â”‚       "depends_on": ["S1"]                          â”‚
â”‚     }                                               â”‚
â”‚   ],                                                â”‚
â”‚   "merge": {                                        â”‚
â”‚     "strategy": "Combine schema + code + tests"     â”‚
â”‚   }                                                 â”‚
â”‚ }                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Validate Plan                                       â”‚
â”‚ validate_plan_structure(                            â”‚
â”‚     plan_data,                                      â”‚
â”‚     allowed_agent_keys=["code_helfer", "architect"],â”‚
â”‚     allowed_engines=["minimax", "smolagent"]        â”‚
â”‚ )                                                   â”‚
â”‚                                                     â”‚
â”‚ Validation Checks:                                  â”‚
â”‚ 1. All subtasks have required fields                â”‚
â”‚ 2. Agent keys exist in agent_manager                â”‚
â”‚ 3. Engine types are supported                       â”‚
â”‚ 4. No circular dependencies (DFS algorithm)         â”‚
â”‚ 5. depends_on references valid task IDs             â”‚
â”‚ 6. parallel_group is valid integer                  â”‚
â”‚                                                     â”‚
â”‚ Raises: PlanValidationError if invalid             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save Plan to Disk                                   â”‚
â”‚ timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")â”‚
â”‚ goal_slug = sanitize_goal("Build REST API")         â”‚
â”‚ filename = f"{timestamp}_{goal_slug}.json"          â”‚
â”‚ plan_path = memory/plans/{filename}                 â”‚
â”‚                                                     â”‚
â”‚ plan_path.write_text(json.dumps(plan_data, indent=2))â”‚
â”‚                                                     â”‚
â”‚ Example: memory/plans/20250118-142305_build-rest-api.jsonâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: EXECUTION                                  â”‚
â”‚ Goal: Execute each subtask                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create ExecutionDispatcher                          â”‚
â”‚ dispatcher = ExecutionDispatcher(                   â”‚
â”‚     plan_path=plan_path,                            â”‚
â”‚     agent_manager=agent_manager,                    â”‚
â”‚     memory_system=memory_system,                    â”‚
â”‚     llm_backends=execution_backends,                â”‚
â”‚     ui=ui,                                          â”‚
â”‚     max_output_tokens=token_limits.execution        â”‚
â”‚ )                                                   â”‚
â”‚                                                     â”‚
â”‚ Loads plan_data from JSON file                      â”‚
â”‚ Extracts: subtasks, merge strategy                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ dispatcher.run()                                    â”‚
â”‚                                                     â”‚
â”‚ Step 1: Group by parallel_group                     â”‚
â”‚ groups = defaultdict(list)                          â”‚
â”‚ for task in subtasks:                               â”‚
â”‚     groups[task["parallel_group"]].append(task)     â”‚
â”‚                                                     â”‚
â”‚ Result:                                             â”‚
â”‚ {                                                   â”‚
â”‚   1: [S1],        # Sequential group                â”‚
â”‚   2: [S2, S3]     # Parallel group                  â”‚
â”‚ }                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Execute Group 1 (Sequential)                        â”‚
â”‚                                                     â”‚
â”‚ for task in group[1]:  # Only S1                    â”‚
â”‚   # Check dependencies                              â”‚
â”‚   for dep_id in task["depends_on"]:                 â”‚
â”‚     if get_task_status(dep_id) != "completed":      â”‚
â”‚       raise ExecutionError("Dependency not met")    â”‚
â”‚                                                     â”‚
â”‚   # Execute                                         â”‚
â”‚   response = _run_subtask(task)                     â”‚
â”‚   task["status"] = "completed"                      â”‚
â”‚   task["result_path"] = save_result(response)       â”‚
â”‚   save_plan()  # Update JSON                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ _run_subtask(S1)                                    â”‚
â”‚                                                     â”‚
â”‚ 1. Get Agent                                        â”‚
â”‚    agent = agent_manager.get("architect")           â”‚
â”‚                                                     â”‚
â”‚ 2. Load Context                                     â”‚
â”‚    history = memory_system.load_relevant_context(   â”‚
â”‚        agent,                                       â”‚
â”‚        hint=S1["objective"],                        â”‚
â”‚        limit=2                                      â”‚
â”‚    )                                                â”‚
â”‚                                                     â”‚
â”‚ 3. Build Prompt                                     â”‚
â”‚    prompt = f"Subtask S1: {S1['objective']}"        â”‚
â”‚    prompt += f"\nNOTES: {S1.get('notes', '')}"      â”‚
â”‚                                                     â”‚
â”‚ 4. Route by Engine                                  â”‚
â”‚    if S1["engine"] == "minimax":                    â”‚
â”‚        response = _invoke_llm(agent, prompt, hist)  â”‚
â”‚    elif S1["engine"] == "smolagent":                â”‚
â”‚        response = _run_smolagent(...)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ _invoke_llm()                                       â”‚
â”‚                                                     â”‚
â”‚ Try backends in fallback order:                     â”‚
â”‚ for backend in [MiniMax, CPU]:                      â”‚
â”‚   try:                                              â”‚
â”‚     response = backend.interface.generate_response( â”‚
â”‚         system_prompt=agent.system_prompt,          â”‚
â”‚         user_prompt=prompt,                         â”‚
â”‚         history=history,                            â”‚
â”‚         max_output_tokens=token_limits.execution    â”‚
â”‚     )                                               â”‚
â”‚     return response  # Success!                     â”‚
â”‚   except Exception as e:                            â”‚
â”‚     continue  # Try next backend                    â”‚
â”‚                                                     â”‚
â”‚ raise ExecutionError("All backends failed")         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save Subtask Result                                 â”‚
â”‚ result_path = memory_system.save_conversation(      â”‚
â”‚     agent=architect,                                â”‚
â”‚     user_message=prompt,                            â”‚
â”‚     assistant_message=response                      â”‚
â”‚ )                                                   â”‚
â”‚                                                     â”‚
â”‚ File: memory/architect/main_20250118-142310.txt     â”‚
â”‚                                                     â”‚
â”‚ Update Plan JSON:                                   â”‚
â”‚ S1["status"] = "completed"                          â”‚
â”‚ S1["result_path"] = str(result_path)                â”‚
â”‚ plan_path.write_text(json.dumps(plan_data))         â”‚
â”‚                                                     â”‚
â”‚ return response                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Execute Group 2 (PARALLEL!)                         â”‚
â”‚                                                     â”‚
â”‚ Tasks: [S2, S3]                                     â”‚
â”‚                                                     â”‚
â”‚ Check dependencies for all tasks:                   â”‚
â”‚ S2.depends_on = ["S1"] â†’ Check S1.status == "completed" âœ“â”‚
â”‚ S3.depends_on = ["S1"] â†’ Check S1.status == "completed" âœ“â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ThreadPoolExecutor                                  â”‚
â”‚ with ThreadPoolExecutor(max_workers=2):             â”‚
â”‚                                                     â”‚
â”‚   futures = {}                                      â”‚
â”‚   for task in [S2, S3]:                             â”‚
â”‚     future = executor.submit(_run_subtask, task)    â”‚
â”‚     futures[future] = task                          â”‚
â”‚                                                     â”‚
â”‚   Thread 1: _run_subtask(S2) â† PARALLEL!            â”‚
â”‚   Thread 2: _run_subtask(S3) â† PARALLEL!            â”‚
â”‚                                                     â”‚
â”‚   Both execute simultaneously!                      â”‚
â”‚   No shared state (except plan JSON with locks)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Wait for Completion                                 â”‚
â”‚                                                     â”‚
â”‚ results = {}                                        â”‚
â”‚ for future in as_completed(futures):                â”‚
â”‚   task = futures[future]                            â”‚
â”‚   task_id = task["id"]                              â”‚
â”‚   try:                                              â”‚
â”‚     response = future.result()  # Get result        â”‚
â”‚     results[task_id] = (task, response)             â”‚
â”‚     ui.status(f"âœ“ Task {task_id} completed")        â”‚
â”‚   except Exception as exc:                          â”‚
â”‚     ui.status(f"âœ— Task {task_id} failed: {exc}")    â”‚
â”‚     executor.shutdown(cancel_futures=True)          â”‚
â”‚     raise ExecutionError(f"Task {task_id} failed")  â”‚
â”‚                                                     â”‚
â”‚ Order of completion: UNPREDICTABLE                  â”‚
â”‚ S3 might finish before S2!                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Display Results SEQUENTIALLY (NEW!)                 â”‚
â”‚                                                     â”‚
â”‚ Sort tasks by ID: [S2, S3]                          â”‚
â”‚                                                     â”‚
â”‚ ui.status(f"ğŸ“Š Ergebnisse (Gruppe 2):")             â”‚
â”‚                                                     â”‚
â”‚ for task in sorted([S2, S3], key=lambda t: t["id"]):â”‚
â”‚   task_id = task["id"]                              â”‚
â”‚   _, response = results[task_id]                    â”‚
â”‚   _display_subtask_result(task_id, task["title"],   â”‚
â”‚                           response)                 â”‚
â”‚                                                     â”‚
â”‚ Output (user sees):                                 â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚ ğŸ“Š Subtask S2: Implement Routes                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚ Flask routes implemented...                         â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚                                                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚ ğŸ“Š Subtask S3: Write Tests                          â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚ Pytest tests created...                             â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: MERGE                                      â”‚
â”‚ Goal: Synthesize all results                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Collect Subtask Results                             â”‚
â”‚                                                     â”‚
â”‚ subtask_results = []                                â”‚
â”‚ for subtask in plan_data["subtasks"]:               â”‚
â”‚   if subtask.get("result_path"):                    â”‚
â”‚     result_file = Path(subtask["result_path"])      â”‚
â”‚     content = result_file.read_text()               â”‚
â”‚     subtask_results.append({                        â”‚
â”‚       "id": subtask["id"],                          â”‚
â”‚       "title": subtask["title"],                    â”‚
â”‚       "result": content[:2000]  # First 2K chars    â”‚
â”‚     })                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MergeMinimaxInterface.merge()                       â”‚
â”‚                                                     â”‚
â”‚ Build Merge Prompt:                                 â”‚
â”‚ """                                                 â”‚
â”‚ Synthesize these subtask results into coherent      â”‚
â”‚ final answer:                                       â”‚
â”‚                                                     â”‚
â”‚ ORIGINAL GOAL: {goal}                               â”‚
â”‚                                                     â”‚
â”‚ SUBTASK RESULTS:                                    â”‚
â”‚                                                     â”‚
â”‚ S1 (Design API Schema):                             â”‚
â”‚ {S1_result}                                         â”‚
â”‚                                                     â”‚
â”‚ S2 (Implement Routes):                              â”‚
â”‚ {S2_result}                                         â”‚
â”‚                                                     â”‚
â”‚ S3 (Write Tests):                                   â”‚
â”‚ {S3_result}                                         â”‚
â”‚                                                     â”‚
â”‚ MERGE STRATEGY: {merge.strategy}                    â”‚
â”‚                                                     â”‚
â”‚ Create comprehensive final response that:           â”‚
â”‚ 1. Summarizes what was accomplished                 â”‚
â”‚ 2. Shows how parts fit together                     â”‚
â”‚ 3. Provides next steps if needed                    â”‚
â”‚ """                                                 â”‚
â”‚                                                     â”‚
â”‚ HTTP POST to MiniMax:                               â”‚
â”‚ max_tokens: token_limits.merge_max_tokens (2048)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Returns Synthesis                               â”‚
â”‚ """                                                 â”‚
â”‚ I've successfully built a complete REST API:        â”‚
â”‚                                                     â”‚
â”‚ 1. API Design (S1):                                 â”‚
â”‚    Created RESTful schema with endpoints for...     â”‚
â”‚                                                     â”‚
â”‚ 2. Implementation (S2):                             â”‚
â”‚    Implemented Flask routes with...                 â”‚
â”‚                                                     â”‚
â”‚ 3. Testing (S3):                                    â”‚
â”‚    Comprehensive test suite with...                 â”‚
â”‚                                                     â”‚
â”‚ The system is production-ready!                     â”‚
â”‚ """                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Display & Save Merge Result                         â”‚
â”‚ ui.stream_prefix("MiniMax-Merge")                   â”‚
â”‚ ui.typing_animation(merged_response)                â”‚
â”‚                                                     â”‚
â”‚ memory_system.save_conversation(                    â”‚
â”‚     agent=merge_agent,                              â”‚
â”‚     user_message="Merge results",                   â”‚
â”‚     assistant_message=merged_response               â”‚
â”‚ )                                                   â”‚
â”‚                                                     â”‚
â”‚ Update Plan Metadata:                               â”‚
â”‚ plan_data["metadata"]["merge_result_path"] = ...    â”‚
â”‚ save_plan()                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: JUDGE (Optional)                           â”‚
â”‚ Goal: Evaluate execution quality                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[See Section 5 for Judge details]
    â†“
Back to Main Loop
```

### DPPM Data Flow Diagram:

```
User: "/plan Build REST API"
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Planner Phase     â”‚
â”‚  (Decompose)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ plan_data.json
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Execution Phase    â”‚
â”‚  (Parallel)        â”‚
â”‚                    â”‚
â”‚  Group 1:          â”‚
â”‚    S1 (seq)        â”‚
â”‚  Group 2:          â”‚
â”‚    S2 â•‘ S3         â”‚
â”‚    (parallel!)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ results
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Merge Phase       â”‚
â”‚  (Synthesis)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ final_answer
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Judge Phase       â”‚
â”‚  (Evaluation)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Tool Execution (Smolagents)

### When engine="smolagent" in subtask:

```
ExecutionDispatcher._run_subtask(task)
    â†“
if task["engine"] == "smolagent":
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ _run_smolagent(task, agent, prompt, history)        â”‚
â”‚                                                     â”‚
â”‚ Extract:                                            â”‚
â”‚ tool_names = task.get("tools", [])                  â”‚
â”‚ # e.g., ["run_aider_task", "read_project_file"]     â”‚
â”‚                                                     â”‚
â”‚ max_steps = task.get("max_steps", 12)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SmolAgentRunner.__init__()                          â”‚
â”‚                                                     â”‚
â”‚ 1. Load tools from tool_registry                    â”‚
â”‚    from selfai.tools.tool_registry import get_tool  â”‚
â”‚    tools = [get_tool(name) for name in tool_names]  â”‚
â”‚                                                     â”‚
â”‚ 2. Convert to smolagents.Tool format                â”‚
â”‚    smol_tools = [tool.to_smol_tool() for tool...]   â”‚
â”‚                                                     â”‚
â”‚ 3. Wrap LLM backend as smolagents.Model             â”‚
â”‚    model = _SelfAIModel(llm_interface)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SmolAgentRunner.run()                               â”‚
â”‚                                                     â”‚
â”‚ agent = ToolCallingAgent(                           â”‚
â”‚     tools=smol_tools,                               â”‚
â”‚     model=model,                                    â”‚
â”‚     max_steps=max_steps                             â”‚
â”‚ )                                                   â”‚
â”‚                                                     â”‚
â”‚ result = agent.run(task=prompt)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ToolCallingAgent (smolagents library)               â”‚
â”‚                                                     â”‚
â”‚ Loop (max max_steps iterations):                    â”‚
â”‚                                                     â”‚
â”‚ Step 1: Call LLM with tools schema                  â”‚
â”‚   messages = [                                      â”‚
â”‚     {"role": "system", "content": system_prompt},   â”‚
â”‚     {"role": "user", "content": task},              â”‚
â”‚   ]                                                 â”‚
â”‚                                                     â”‚
â”‚   llm_response = model.generate(messages, tools)    â”‚
â”‚                                                     â”‚
â”‚ Step 2: Parse response for tool calls               â”‚
â”‚   if "[TOOL_CALL]" in llm_response:                 â”‚
â”‚     # Extract: {"tool": "run_aider_task", ...}      â”‚
â”‚     tool_call = parse_tool_call(llm_response)       â”‚
â”‚                                                     â”‚
â”‚ Step 3: Execute tool                                â”‚
â”‚     tool = get_tool(tool_call["tool"])              â”‚
â”‚     result = tool.run(**tool_call["arguments"])     â”‚
â”‚                                                     â”‚
â”‚ Step 4: Inject result back into conversation        â”‚
â”‚     messages.append({                               â”‚
â”‚       "role": "tool",                               â”‚
â”‚       "content": result,                            â”‚
â”‚       "tool_call_id": tool_call["id"]               â”‚
â”‚     })                                              â”‚
â”‚                                                     â”‚
â”‚ Step 5: Continue loop                               â”‚
â”‚   if "[FINAL_ANSWER]" in llm_response:              â”‚
â”‚     break  # Done!                                  â”‚
â”‚                                                     â”‚
â”‚ Return: Final answer text                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tool Execution Example: run_aider_task              â”‚
â”‚                                                     â”‚
â”‚ def run_aider_task(task_description, files="", ...) â”‚
â”‚                                                     â”‚
â”‚   subprocess.run([                                  â”‚
â”‚     "aider",                                        â”‚
â”‚     "--model", "openai/MiniMax-M2",                 â”‚
â”‚     "--message", task_description,                  â”‚
â”‚     "--yes",  # Auto-accept changes                 â”‚
â”‚     *files.split(",")                               â”‚
â”‚   ],                                                â”‚
â”‚   cwd=project_root,                                 â”‚
â”‚   capture_output=True,                              â”‚
â”‚   text=True,                                        â”‚
â”‚   timeout=180                                       â”‚
â”‚   )                                                 â”‚
â”‚                                                     â”‚
â”‚   Aider Process:                                    â”‚
â”‚   1. Reads specified files                          â”‚
â”‚   2. Calls MiniMax API with file content + task     â”‚
â”‚   3. Generates code changes                         â”‚
â”‚   4. Applies edits to files                         â”‚
â”‚   5. Creates git commit                             â”‚
â”‚   6. Returns git diff as result                     â”‚
â”‚                                                     â”‚
â”‚   return json.dumps({                               â”‚
â”‚     "status": "success",                            â”‚
â”‚     "diff": result.stdout,                          â”‚
â”‚     "files_changed": files                          â”‚
â”‚   })                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Result returned to ToolCallingAgent
    â†“
Agent injects result, calls LLM again
    â†“
Loop continues until FINAL_ANSWER
    â†“
Return to ExecutionDispatcher
```

### Tool Call Flow Diagram:

```
Subtask (engine=smolagent)
    â†“
SmolAgentRunner
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ToolCallingAgent     â”‚
â”‚                      â”‚
â”‚ Loop:                â”‚
â”‚   1. Call LLM        â”‚
â”‚   2. Parse response  â”‚
â”‚   3. If tool call:   â”‚
â”‚      â”œâ”€â†’ Execute toolâ”‚
â”‚      â””â”€â†’ Inject resultâ”‚
â”‚   4. If final answer:â”‚
â”‚      â””â”€â†’ Break       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tool Execution       â”‚
â”‚ (subprocess)         â”‚
â”‚                      â”‚
â”‚ Aider/OpenHands/etc  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. External Process Communication

### A. Aider Tool

```
run_aider_task(task_description, files, ...)
    â†“
subprocess.run([
    "aider",
    "--model", "openai/MiniMax-M2",
    "--message", task_description,
    "--yes",
    "file1.py", "file2.py"
],
cwd=project_root,
capture_output=True,
timeout=180
)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aider Process                                       â”‚
â”‚                                                     â”‚
â”‚ 1. Parse command line args                          â”‚
â”‚ 2. Read specified files                             â”‚
â”‚ 3. Build prompt:                                    â”‚
â”‚    "Edit these files to: {task_description}"        â”‚
â”‚ 4. HTTP POST to MiniMax API                         â”‚
â”‚    POST https://api.minimax.chat/...                â”‚
â”‚    {                                                â”‚
â”‚      "model": "abab6.5s-chat",                      â”‚
â”‚      "messages": [                                  â”‚
â”‚        {                                            â”‚
â”‚          "role": "system",                          â”‚
â”‚          "content": "You are aider..."              â”‚
â”‚        },                                           â”‚
â”‚        {                                            â”‚
â”‚          "role": "user",                            â”‚
â”‚          "content": task_description +              â”‚
â”‚                     file_contents                   â”‚
â”‚        }                                            â”‚
â”‚      ]                                              â”‚
â”‚    }                                                â”‚
â”‚ 5. Parse LLM response for edits                     â”‚
â”‚ 6. Apply edits to files                             â”‚
â”‚ 7. Create git commit                                â”‚
â”‚ 8. Output diff to stdout                            â”‚
â”‚ 9. Exit with code 0                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
stdout captured by run_aider_task()
    â†“
return json.dumps({
    "status": "success",
    "diff": stdout,
    "files_changed": files
})
```

### B. Gemini Judge (CLI)

```
GeminiJudge.evaluate_task(...)
    â†“
Build evaluation prompt
    â†“
subprocess.run([
    "/path/to/gemini",
    "-p", "Respond ONLY with valid JSON"
],
input=evaluation_prompt,
capture_output=True,
stderr=subprocess.DEVNULL  # Suppress startup logs!
)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gemini CLI Process                                  â”‚
â”‚                                                     â”‚
â”‚ 1. Parse -p flag (prompt mode)                      â”‚
â”‚ 2. Read stdin (evaluation_prompt)                   â”‚
â”‚ 3. HTTP POST to Gemini API                          â”‚
â”‚    POST https://generativelanguage.googleapis.com/  â”‚
â”‚    {                                                â”‚
â”‚      "contents": [{                                 â”‚
â”‚        "parts": [{                                  â”‚
â”‚          "text": evaluation_prompt                  â”‚
â”‚        }]                                           â”‚
â”‚      }]                                             â”‚
â”‚    }                                                â”‚
â”‚ 4. Get JSON response from API                       â”‚
â”‚ 5. Write to stdout                                  â”‚
â”‚ 6. Exit                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
stdout captured by GeminiJudge
    â†“
Parse JSON, create JudgeScore dataclass
    â†“
return JudgeScore(...)
```

### C. OpenHands Tool

```
run_openhands_task(task_description, ...)
    â†“
subprocess.run([
    "poetry", "run", "python",
    "openhands/core/main.py",
    "-t", task_description,
    "-c", "openai/MiniMax-M2",
    "-m", "10"  # max iterations
],
cwd=openhands_dir,
capture_output=True,
timeout=600  # 10 minutes
)
    â†“
OpenHands process (autonomous agent)
    â†“
Similar to Smolagents but in separate process
    â†“
Returns: stdout with results
```

---

## 6. Memory System

### Filesystem Structure:

```
memory/
â”œâ”€â”€ plans/
â”‚   â”œâ”€â”€ 20250118-140000_build-rest-api.json
â”‚   â”œâ”€â”€ 20250118-150000_refactor-auth.json
â”‚   â””â”€â”€ 20250118-160000_add-tests.json
â”‚
â”œâ”€â”€ code_helfer/  # Category 1
â”‚   â”œâ”€â”€ main_20250118-140000.txt
â”‚   â”œâ”€â”€ main_20250118-140500.txt
â”‚   â””â”€â”€ main_20250118-141000.txt
â”‚
â”œâ”€â”€ architect/  # Category 2
â”‚   â”œâ”€â”€ main_20250118-140100.txt
â”‚   â””â”€â”€ main_20250118-140600.txt
â”‚
â””â”€â”€ general/  # Default category
    â””â”€â”€ main_20250118-140200.txt
```

### Memory File Format:

```
---
Agent: Code Helper
AgentKey: code_helfer
Workspace: main
Timestamp: 2025-01-18 14:00:00
Tags: python, debugging, error
---
System Prompt:
You are a helpful coding assistant specializing in...
---
User:
How do I fix this Python error: TypeError...
---
SelfAI:
This error occurs because you're trying to...

The solution is to:
1. Check the variable type
2. Add type conversion
3. Validate inputs

Here's the fixed code:
```python
...
```
```

### Plan File Format (JSON):

```json
{
  "subtasks": [
    {
      "id": "S1",
      "title": "Design API Schema",
      "objective": "Define REST endpoints...",
      "agent_key": "architect",
      "engine": "minimax",
      "parallel_group": 1,
      "depends_on": [],
      "status": "completed",
      "result_path": "memory/architect/main_20250118-142310.txt",
      "error": null,
      "notes": ""
    }
  ],
  "merge": {
    "strategy": "Combine all results...",
    "steps": [...]
  },
  "metadata": {
    "goal": "Build REST API",
    "planner_provider": "minimax-planner",
    "planner_model": "abab6.5s-chat",
    "merge_provider": "minimax-merger",
    "merge_result_path": "memory/architect/main_20250118-142400.txt",
    "created_at": "2025-01-18T14:23:05",
    "fallback": false
  }
}
```

---

## 7. Context Loading

### NEW: Time-Based Context Window

```python
# memory_system.py
def load_relevant_context(self, agent, current_text, limit=3):
    # Step 1: Get candidate files
    categories = agent.memory_categories
    candidate_files = self._get_candidate_files(categories)

    # Step 2: TIME FILTER (NEW!)
    import time
    cutoff_time = time.time() - (self.context_window_minutes * 60)
    candidate_files = [
        f for f in candidate_files
        if f.stat().st_mtime >= cutoff_time
    ]

    if not candidate_files:
        return []  # No files in window!

    # Step 3: Classify current task
    classification = classify_task(current_text, agent.key)
    expected_tags = classification.tags

    # Step 4: Score files by tag relevance
    scored_files = []
    for file_path in candidate_files:
        parsed = self._parse_memory_file(file_path)
        file_tags = parsed.get("tags", [])

        score = calculate_relevance(expected_tags, file_tags)
        scored_files.append({
            "path": file_path,
            "parsed": parsed,
            "score": score,
            "mtime": file_path.stat().st_mtime
        })

    # Step 5: Filter by threshold
    threshold = 0.35
    relevant = [f for f in scored_files if f["score"] >= threshold]

    # Step 6: Fallback if no relevant
    if not relevant:
        relevant = scored_files[:limit]

    # Step 7: Sort by (score, mtime) DESC
    relevant.sort(key=lambda f: (f["score"], f["mtime"]), reverse=True)

    # Step 8: Take top N
    selected = relevant[:limit]

    # Step 9: Re-sort chronologically (oldest first)
    selected.sort(key=lambda f: f["mtime"])

    # Step 10: Build context messages
    context = []
    for entry in selected:
        parsed = entry["parsed"]
        context.append({"role": "user", "content": parsed["user"]})
        context.append({"role": "assistant", "content": parsed["assistant"]})

    return context
```

### Context Timeline:

```
Session Start: 14:00
Window: 30 minutes
Current Time: 14:35

Files:
â”œâ”€â”€ 14:00 - RAG discussion     â† Outside window (14:05 cutoff)
â”œâ”€â”€ 14:10 - Python basics      â† Inside window âœ“
â”œâ”€â”€ 14:20 - Django tutorial    â† Inside window âœ“
â”œâ”€â”€ 14:30 - Flask API          â† Inside window âœ“
â””â”€â”€ 14:35 - Current prompt

Cutoff Calculation:
cutoff = 14:35 - 0:30 = 14:05

Filter Result:
RAG (14:00): 14:00 >= 14:05? NO âœ— â†’ FILTERED OUT
Python (14:10): 14:10 >= 14:05? YES âœ“ â†’ KEEP
Django (14:20): 14:20 >= 14:05? YES âœ“ â†’ KEEP
Flask (14:30): 14:30 >= 14:05? YES âœ“ â†’ KEEP

Context Loaded: [Python, Django, Flask]
RAG NOT in context! Clean! âœ“
```

---

## 8. Parallel Execution

### Thread Pool Pattern:

```python
# execution_dispatcher.py
with ThreadPoolExecutor(max_workers=len(tasks)) as executor:
    # Submit all tasks
    futures = {}
    for task in tasks:
        future = executor.submit(_run_subtask, task)
        futures[future] = task

    # Wait for completion (order: UNPREDICTABLE)
    results = {}
    for future in as_completed(futures):
        task = futures[future]
        task_id = task["id"]

        try:
            response = future.result()  # Blocking
            results[task_id] = (task, response)
            ui.status(f"âœ“ Task {task_id} completed")
        except Exception as exc:
            ui.status(f"âœ— Task {task_id} failed: {exc}")
            executor.shutdown(cancel_futures=True)
            raise

    # Display in ORDER (by task ID)
    for task in sorted(tasks, key=lambda t: t["id"]):
        task_id = task["id"]
        if task_id in results:
            _, response = results[task_id]
            display_result(task_id, response)
```

### Thread Safety:

| Component | Thread-Safe? | Notes |
|-----------|--------------|-------|
| MemorySystem | âœ“ Yes | Filesystem locks (OS-level) |
| Plan JSON | âœ“ Yes | Atomic writes |
| TerminalUI | âœ— No | But output is serialized after threads complete |
| LLM Interfaces | âœ“ Yes | Each thread has own HTTP connection |
| AgentManager | âœ“ Yes | Read-only during execution |

### Execution Timeline:

```
Group 1 (Sequential):
[14:00:00] S1 starts
[14:00:30] S1 completes
           â†“
Group 2 (Parallel):
[14:00:31] S2 starts (Thread 1)
[14:00:31] S3 starts (Thread 2)  â† Same time!
           â•‘
[14:00:45] S3 completes (Thread 2 faster)
[14:00:50] S2 completes (Thread 1 slower)
           â†“
Display (Sequential):
[14:00:51] Show S2 result (sorted by ID)
[14:00:51] Show S3 result
```

---

## 9. Error Handling & Retry

### Backend Fallback Chain:

```python
def _invoke_llm(agent, prompt, history):
    last_error = None

    # Try each backend in order
    for backend in [MiniMax, CPU]:
        try:
            response = backend.generate_response(
                system_prompt=agent.system_prompt,
                user_prompt=prompt,
                history=history
            )
            return response  # Success!

        except Exception as exc:
            last_error = exc
            ui.status(f"Backend {backend.name} failed: {exc}", "warning")
            continue  # Try next backend

    # All backends failed
    raise ExecutionError(f"All backends failed. Last: {last_error}")
```

### Retry Logic:

```python
# execution_dispatcher.py
retry_attempts = 2
retry_delay = 5.0

for attempt in range(retry_attempts + 1):  # 0, 1, 2
    try:
        response = llm_interface.generate_response(...)
        return response  # Success!

    except Exception as exc:
        last_exception = exc

        if attempt < retry_attempts:
            ui.status(f"Attempt {attempt+1} failed. Retrying in {retry_delay}s...", "warning")
            time.sleep(retry_delay)
            continue

        # All retries exhausted
        raise ExecutionError(f"Failed after {retry_attempts+1} attempts: {exc}")
```

### Error Flow:

```
LLM Call
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Try MiniMax     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€ Success? â†’ Return
     â”‚
     â”œâ”€ HTTP Error?
     â”‚  â”œâ”€ Retry 1 (5s delay)
     â”‚  â”œâ”€ Retry 2 (5s delay)
     â”‚  â””â”€ Still failing?
     â”‚     â””â”€â†’ Try next backend
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Try CPU         â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€ Success? â†’ Return
     â”‚
     â”œâ”€ Error?
     â”‚  â”œâ”€ Retry 1
     â”‚  â”œâ”€ Retry 2
     â”‚  â””â”€ Still failing?
     â”‚     â””â”€â†’ No more backends
     â”‚
     â†“
ExecutionError raised
```

---

## 10. Data Structures

### A. Agent (Dataclass)

```python
@dataclass
class Agent:
    key: str                      # "code_helfer"
    display_name: str             # "Code Helper"
    description: str              # "Helps with coding tasks"
    system_prompt: str            # Full prompt text (multiline)
    memory_categories: List[str]  # ["python", "debugging"]
    workspace_slug: str           # "main"
```

**Storage:** In-memory (`AgentManager.agents: List[Agent]`)
**Source:** `agents/{key}/` directory files
**Lifetime:** Entire session (loaded at startup)

### B. Plan Data (Dict/JSON)

```python
{
    "subtasks": [
        {
            "id": str,              # "S1", "S2", ...
            "title": str,           # "Design API Schema"
            "objective": str,       # Detailed task description
            "agent_key": str,       # "architect", "code_helfer"
            "engine": str,          # "minimax", "smolagent"
            "parallel_group": int,  # 1, 2, 3, ...
            "depends_on": List[str], # ["S1"], ["S2", "S3"]
            "status": str,          # "pending", "running", "completed", "failed"
            "result_path": str|None, # "memory/architect/main_20250118.txt"
            "error": str|None,      # Error message if failed
            "notes": str,           # Additional context
            "tools": List[str]|None, # ["run_aider_task"] (for smolagent)
            "max_steps": int|None   # Max tool iterations
        }
    ],
    "merge": {
        "strategy": str,            # How to combine results
        "steps": List[Dict]         # Merge steps (optional)
    },
    "metadata": {
        "goal": str,                # Original user goal
        "planner_provider": str,    # "minimax-planner"
        "planner_model": str,       # "abab6.5s-chat"
        "merge_provider": str,      # "minimax-merger"
        "merge_result_path": str,   # Path to merge result
        "created_at": str,          # ISO timestamp
        "fallback": bool            # True if fallback plan
    }
}
```

**Storage:** Filesystem (`memory/plans/{timestamp}_{goal}.json`)
**Updates:** Plan JSON is updated after each subtask (atomic writes)
**Access:** Loaded/saved by `ExecutionDispatcher`

### C. LLM Backend (Dict)

```python
{
    "interface": MinimaxInterface(...),  # Interface instance
    "label": "MiniMax",                  # Display name
    "name": "minimax",                   # Internal identifier
    "type": "cloud"                      # "cloud", "local", "npu"
}
```

**Storage:** In-memory (`execution_backends: List[Dict]`)
**Access:** Indexed list (fallback order = list order)
**Lifetime:** Entire session

### D. Token Limits (Dataclass)

```python
@dataclass
class TokenLimits:
    planner_max_tokens: int = 768
    execution_max_tokens: int = 512
    merge_max_tokens: int = 2048
    tool_creation_max_tokens: int = 1024
    error_correction_max_tokens: int = 1024
    selfimprove_max_tokens: int = 2048
    chat_max_tokens: int = 1024
```

**Storage:** In-memory (runtime only)
**Control:** `/tokens`, `/extreme` commands
**Lifetime:** Current session (resets on restart)

### E. Memory Context (List[Dict])

```python
[
    {
        "role": "user",
        "content": "How do I fix this error?"
    },
    {
        "role": "assistant",
        "content": "The error occurs because..."
    },
    {
        "role": "user",
        "content": "Thanks! What about..."
    },
    {
        "role": "assistant",
        "content": "For that, you can..."
    }
]
```

**Source:** `load_relevant_context()` from memory files
**Usage:** Passed to LLM as conversation history
**Format:** OpenAI-compatible messages format

---

## Summary: Communication Patterns

### Synchronous (Main Thread):
- User Input â†’ Main Loop
- Command Parsing â†’ Command Handlers
- Memory Load/Save â†’ Filesystem I/O
- Plan Validation â†’ In-Memory
- Output Display â†’ Terminal

### HTTP (Async I/O in sync context):
- LLM Interfaces â†’ Cloud APIs (MiniMax, etc.)
- Streaming via SSE (Server-Sent Events)
- Request/Response JSON

### Multi-Threading (Parallel Execution):
- ExecutionDispatcher spawns threads
- Each thread runs subtask independently
- Results collected, displayed sequentially

### Sub-Processes (External Tools):
- Aider: `subprocess.run(["aider", ...])`
- OpenHands: `subprocess.run(["poetry", "run", ...])`
- Gemini CLI: `subprocess.run(["/path/to/gemini", ...])`

### Filesystem (Persistent State):
- Plans: `memory/plans/{timestamp}_{goal}.json`
- Conversations: `memory/{category}/{agent}_{timestamp}.txt`
- Agent Config: `agents/{agent_key}/{files}`

---

**No Message Queue, No Event Bus, No WebSockets between components!**

Everything is **direct function calls** with **Filesystem as shared state** and **HTTP for external APIs**.

**Simple, Transparent, Debuggable.** ğŸ¯
