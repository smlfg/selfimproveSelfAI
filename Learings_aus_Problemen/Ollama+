**TEIL 1 Â· Code-Ablauf-Dokumentation**

## selfai Runtime

### ğŸ“ Dateien
- `selfai/selfai.py`
- `config.yaml`, `config_loader.py`
- `selfai/ui/terminal_ui.py`
- `selfai/core/*`

### ğŸ—ï¸ Aufbau / Struktur
- `main()` orchestriert Start-up & Event Loop
- Hilfsfunktionen: `_show_system_resources`, `_build_planner_context`, `_execute_merge_phase`, u.â€¯a.
- Zustandsdateien: `planner_state.json`, `merge_state.json`
- AbhÃ¤ngigkeiten: `config_loader.AppConfig`, `AgentManager`, `AnythingLLMInterface`, `MemorySystem`, `ExecutionDispatcher`, diverse Ollama-Interfaces

### â–¶ï¸ Ablauf (Schritt-fÃ¼r-Schritt)

**Initialisierung**
1. `python -m selfai.selfai` â†’ `main()`
2. `load_configuration()` liest `config.yaml`, `.env`, normalisiert Planner/Merge-Provider
3. UI-Setup (`TerminalUI.banner`, `status`)
4. `AgentManager` lÃ¤dt Agent-Definitionen aus `agents/`
5. `MemorySystem` initialisiert Speicherverzeichnis und Planordner
6. Planner-Provider (Ollama lokal/cloud) aufgebaut & Healthcheck
7. Merge-Provider (AnythingLLM default + optionale Ollama-Backends) vorbereitet
8. AnythingLLM-Verbindung (Fallbacks: QNN, CPU GGUF)
9. Aktiven Agent setzen (Default aus Konfig oder erster Eintrag)
10. Systemressourcen anzeigen (RAM/CPU psutil)
11. CLI Hinweis auf Commands (`/plan`, `/planner`, `/switch`)

**Request-Verarbeitung (freier Chat)**
1. Nutzer-Eingabe (`input("Du:")`)
2. Kommandoparsing (`/planner`, `/plan`, `/switch`, `quit`)
3. Bei freiem Prompt: aktiver Agent via `AgentManager`
4. Memory-Kontext laden: `MemorySystem.load_relevant_context(agent)`
5. Systemprompt + Kontext + Userprompt â†’ AnythingLLM (Streaming via `stream_generate_response`)
6. UI-Streaming (`TerminalUI.stream_prefix` + `streaming_chunk`)
7. GesprÃ¤ch speichern: `MemorySystem.save_conversation`
8. Spinner/Status aktualisieren, Loop fortsetzen

**Planer Workflow (`/plan <Ziel>`)**
1. Planner-Kontext erstellen (Agenten + Memory-Zusammenfassung)
2. Provider-Round-Robin (zuletzt aktiver zuerst)
3. Streamender Ollama-Aufruf (`PlannerOllamaInterface.plan`)
4. `validate_plan_logic` â†’ Warnungen/Fehler anzeigen
5. UI-BestÃ¤tigung (Plan Ã¼bernehmen, AusfÃ¼hren)
6. Plan speichern (`MemorySystem.save_plan`)
7. Subtasks ausfÃ¼hren (`ExecutionDispatcher` mit Streaming, Retries)
8. Merge-Backend wÃ¤hlen (`_select_merge_backend`)
9. `_execute_merge_phase` sammelt Subtask-Outputs, erzeugt Merge-Prompt, streamt Ergebnis Ã¼ber gewÃ¤hltes Backend
10. Metadaten zurÃ¼ck in Plan-Datei (Provider, Agent, Ergebnis-Pfad)

### ğŸ”— AbhÃ¤ngigkeiten

**Basis-Layer**
- Python â‰¥3.10 (Projekt nutzt Pattern Matching, Typ-Hints)
- `httpx`, `psutil`, `python-dotenv`, `PyYAML`
- Optionale Hardware: Qualcomm NPU (QNN), ansonsten CPU GGUF via llama.cpp

**Integration-Layer**
- AnythingLLM API (`/api/chat`, Stream API)
- Lokale Ollama-Instanz (`/api/generate`, `/api/tags`)
- Dateisystem fÃ¼r Memory & PlÃ¤ne (`selfai/memory/`)

**Request-Chain**
User Input  
â†’ Command Parser (`selfai.py`)  
â†’ AgentManager (`agent_manager.get`)  
â†’ MemorySystem (`load_relevant_context`)  
â†’ LLM Interface (AnythingLLM/NPU/CPU)  
â†’ Streaming UI (`TerminalUI`)  
â†’ Memory Save (`MemorySystem.save_conversation`)  
â†’ Optional: Plan Execution (`ExecutionDispatcher`)  
â†’ Merge Backend (`MergeOllamaInterface` oder AnythingLLM)

### âš ï¸ Kritische Dependencies

| Komponente              | Fallback                          | KritikalitÃ¤t |
|-------------------------|-----------------------------------|--------------|
| AnythingLLM API         | QNN/CPU GGUF (eingeschrÃ¤nkt)      | CRITICAL     |
| Ollama Planner Provider | Weitere Provider, sonst disabled  | HIGH         |
| Agent-Definitionen      | Fehlende Agenten â†’ Programm Ende  | HIGH         |
| `config.yaml`           | Template Defaults (eingeschrÃ¤nkt) | MEDIUM       |
| Memory-Verzeichnis      | Auto-Anlage                       | MEDIUM       |

### ğŸš¨ Error Handling
- AnythingLLM/Ollama HTTP â†’ `PlannerError`, UI-Warnungen, Provider-Switch
- Stream-Fehler â†’ UI Warnung, Fallback auf Block-Modus
- ExecutionDispatcher retryt Subtasks (max 3 Gesamtversuche)
- Dateischreibfehler â†’ UI-Fehler, Abbruch des Plans
- Merge ohne Resultate â†’ Warnung, kein Merge-Call

### ğŸ“Š Performance (aktueller Stand)
- Instrumentierung noch nicht implementiert; Messpunkte vorgesehen:
  - Planner Roundtrip (per Provider)
  - Subtask Runtime & Retry Count
  - Merge Latenz
  - Memory Load/Speicherzeit

---

## config_loader

### ğŸ“ Dateien
- `config_loader.py`
- `config.yaml`, `config.yaml.template`, `config_extended.yaml`

### ğŸ—ï¸ Aufbau
- Dataclasses: `AppConfig`, `PlannerConfig`, `MergeConfig`, `PlannerProviderConfig`, â€¦
- `_normalize_config` unterstÃ¼tzt altes & neues Schema
- `_resolve_env_template` ersetzt `${VAR}` Platzhalter
- `load_configuration` liest YAML + Umgebungsvariablen, validiert EintrÃ¤ge

### â–¶ï¸ Ablauf
1. `.env` lesen (`load_dotenv`)
2. YAML parsen
3. Normalize: sicherstellen, dass Planner/Merge Provider Listen existieren
4. API-Key AuflÃ¶sung (ENV > Datei)
5. Dataclasses instanziieren; Headers aus Umgebungsvariablen injizieren
6. Validierung: Pflichtfelder (`base_url`, `model`), Datentypen, Standardwerte

### ğŸ”— AbhÃ¤ngigkeiten
- `python-dotenv`, `PyYAML`, Standardbibliothek
- `.env` mit `API_KEY`
- `config.yaml` (oder Template fallback)

### âš ï¸ Kritisch
- Fehlender API Key â†’ `ValueError` beim Start
- Invalid YAML â†’ Programmabbruch mit Fehlerhinweis

---

## Terminal UI

### ğŸ“ Dateien
- `selfai/ui/terminal_ui.py`

### ğŸ—ï¸ Aufbau
- Klasse `TerminalUI` (Statusmeldungen, Spinner, Streaming, AuswahlmenÃ¼s)
- UnterstÃ¼tzt Farben (Colorama fallback)
- Methoden: `status`, `start_spinner`, `stream_prefix`, `streaming_chunk`, `choose_option`, `confirm_plan`, `list_agents`, `show_plan`

### â–¶ï¸ Ablauf
- Wird in `main()` instanziiert
- Alle User-facing Messages laufen Ã¼ber `TerminalUI`
- Stream-Ausgaben werden per `stream_prefix` + `streaming_chunk` realisiert
- Auswahldialog (Merge/Planner Provider) via `choose_option`

### ğŸ”— AbhÃ¤ngigkeiten
- Optional `colorama` (Windows)
- Standardbibliothek (threading, time, itertools)

---

## Agent Management

### ğŸ“ Dateien
- `selfai/core/agent_manager.py`
- `selfai/core/agent.py`
- `agents/*.yaml`

### ğŸ—ï¸ Aufbau
- `Agent` Dataclass (Key, Display Name, System Prompt, Workspace, Memory-Kategorien, Farbe, Beschreibung)
- `AgentManager` lÃ¤dt YAML-Dateien, hÃ¤lt aktive Auswahl, wechselt Agenten
- Methoden: `list_agents`, `switch_agent`, `get`, `active_agent`

### â–¶ï¸ Ablauf
1. Beim Start: Verzeichnis `agents/` scannen, YAML laden
2. Aktiver Agent per Konfig oder Standard
3. `switch_agent` erlaubt Wechsel via Name/Index
4. `list_agents` fÃ¼r UI-Anzeige, inklusive Farbe & Workspace

### ğŸ”— AbhÃ¤ngigkeiten
- `yaml.safe_load`
- Struktur der Agent YAMLs (Key, workspace_slug, system_prompt, Memory-Kategorien)

---

## Memory System

### ğŸ“ Dateien
- `selfai/core/memory_system.py`

### ğŸ—ï¸ Aufbau
- `MemorySystem` verwaltet Datenpfade: GesprÃ¤che (`memory/conversations/`), PlÃ¤ne (`memory/plans/`)
- Funktionen: `save_conversation`, `load_relevant_context`, `save_plan`, `memory_dir` properties
- Plant weitere Features: Kategorisierung, Indizes

### â–¶ï¸ Ablauf
1. Konstruktor legt Basisordner an (falls nicht vorhanden)
2. `save_conversation` speichert JSON mit Agent, Prompt, Antwort
3. `load_relevant_context` (aktuell heuristisch â€“ lÃ¤dt letzte EintrÃ¤ge)
4. `save_plan` legt Plan-Datei (Timestamp + slug) im Plan-Ordner ab

### ğŸ”— AbhÃ¤ngigkeiten
- `pathlib`, `json`
- Dateisystemzugriff mit Schreibrechten

---

## AnythingLLM Interface

### ğŸ“ Dateien
- `selfai/core/anythingllm_interface.py`

### ğŸ—ï¸ Struktur
- Klasse `AnythingLLMInterface`
- Methoden: `__init__`, `chat` (non-stream), `stream_generate_response`
- HTTP via `httpx` (Sitzung), Streaming per `client.stream`

### â–¶ï¸ Ablauf
- Instanziierung mit Base URL, API Key, Workspace Slug
- `stream_generate_response`: Sendet `{"message": ..., "systemPrompt": ..., â€¦}` an `/api/chat`
- Erwartet Text-Stream; yields Chunks an Aufrufer
- Fehler â†’ `raise AnythingLLMError`

### ğŸ”— AbhÃ¤ngigkeiten
- `httpx`
- AnythingLLM Server mit gÃ¼ltigem Workspace & Key

---

## Planner (Ollama)

### ğŸ“ Dateien
- `selfai/core/planner_ollama_interface.py`
- `selfai/core/planner_validator.py`

### ğŸ—ï¸ Struktur
- `PlannerOllamaInterface`: `plan`, `_build_prompt`, `_parse_plan`, `healthcheck`
- `PlannerContext` Dataclass (Agentliste, Memory-Summary)
- Validator: `validate_plan_structure`, `validate_plan_logic`

### â–¶ï¸ Ablauf
1. Prompt-Template injiziert Ziel, Agenten, Erinnerungen, Regeln (DPPM, JSON Schema, Clarification)
2. HTTP POST `{"model": ..., "prompt": ..., "stream": True, "format": "json"}` an `/api/generate`
3. Streaming-Chunks â†’ Progress Callback (CLI print)
4. Rohantwort JSON-parsen, Code-Fences strippen, `END_OF_PLAN` beachten
5. Validierung: Struktur (IDs, Agent Keys, Merge Steps), Logik (Duplikate, Dependencies, Merge-Strategie)
6. Fehler â†’ `PlannerError` mit Plan-Snapshot fÃ¼r Debug

### ğŸ”— AbhÃ¤ngigkeiten
- Ollama Server mit passender Modell-ID
- `httpx`
- Agent Keys/Engines aus SelfAI

---

## Execution Dispatcher

### ğŸ“ Dateien
- `selfai/core/execution_dispatcher.py`

### ğŸ—ï¸ Struktur
- Klasse `ExecutionDispatcher`
- Kernmethoden: `run`, `_run_subtask`, `_invoke_llm`, `_update_task_status`, `_save_plan`
- Arbeitet sequentiell (Parallelisierung Phaseâ€¯B TODO)

### â–¶ï¸ Ablauf
1. LÃ¤dt Plan JSON
2. Initialisiert Statusfelder (`pending`)
3. FÃ¼r jeden Subtask:
   - Agent & Engine validieren
   - Prompt bauen (`Subtask {id}: objective\nNOTES: ...`)
   - LLM Call via AnythingLLM (Streaming fallback)
   - Ergebnis speichern (MemorySystem)
   - Status aktualisieren (`running` â†’ `completed`/`failed`)
4. Fehler â†’ `ExecutionError`, stoppt AusfÃ¼hrung

### ğŸ”— AbhÃ¤ngigkeiten
- AnythingLLM Interface
- MemorySystem zum Speichern
- TerminalUI fÃ¼r Statusmeldungen

---

## Merge Backend

### ğŸ“ Dateien
- `selfai/core/merge_ollama_interface.py` (Ollama Merge)
- `selfai/selfai.py` (`_execute_merge_phase`, `_select_merge_backend`)

### ğŸ—ï¸ Struktur
- `MergeOllamaInterface`: Streaming/Non-stream Chat fÃ¼r Resultat-ZusammenfÃ¼hrung
- `_execute_merge_phase` koordiniert Ergebnisaggregation, Prompt, Streaming, Metadaten

### â–¶ï¸ Ablauf
1. Sammeln aller `result_path` aus Subtasks
2. Zusammenbau finaler Merge-Prompt (Strategie, Schritte, Outputs)
3. Auswahl Merge-Backend (AnythingLLM default, optional Ollama)
4. Streaming mit UI Feedback; Fallback Non-Stream
5. Speicherung Merge-Ergebnis + Metadaten (Provider, Model, Timeout, Tokens)

### ğŸ”— AbhÃ¤ngigkeiten
- Merge Provider Konfiguration (Ollama URL/Model oder AnythingLLM)
- MemorySystem fÃ¼r Resultat-Load & Save

---

**TEIL 2 Â· Dependencies & Fundament**

### Ebene 1 Â· Hardware & Runtime
- Python â‰¥3.10 im virtuellen Environment
- Optional Qualcomm NPU (QNN) mit kompatiblen Modellen
- Mind. 16â€¯GB RAM empfohlen (Planner + Merge LLM parallel)

### Ebene 2 Â· System-Level
- `httpx` (HTTP)
- `psutil` (Systemmonitor)
- `python-dotenv`, `PyYAML`
- `llama-cpp-python` (CPU/QNN Fallback)
- Lokaler AnythingLLM Dienst (`http://localhost:3001`)
- Lokale Ollama Instanz (`http://localhost:11434`)

### Ebene 3 Â· Python Requirements (Auszug)
- `httpx>=0.26`
- `python-dotenv>=1.0`
- `PyYAML>=6.0`
- `tabulate`, `numpy`, `pyarrow` (fÃ¼r weitere Tools)
- `llama-cpp-python==0.3.16` (Wheel Build â†’ Visual Studio Build Tools erforderlich auf Windows ARM)

### Ebene 4 Â· SelfAI Core Module
- `config_loader` (Konfig)
- `agent_manager`, `agent`
- `memory_system`
- `anythingllm_interface`, `local_llm_interface`, `npu_llm_interface`
- `planner_ollama_interface`, `merge_ollama_interface`, `execution_dispatcher`
- `planner_validator`, `tool_calling_ollama_interface`

### Ebene 5 Â· Daten & Konfiguration
- `config.yaml` (API Keys, Planner/Merge Provider)
- `agents/*.yaml` (Agent-Spezifika)
- `memory/` (GesprÃ¤chsdaten, PlÃ¤ne, ZustÃ¤nde)
- `planner_state.json`, `merge_state.json` (Persistente Auswahl)
- Optionale `.env` fÃ¼r Secrets

### Ebene 6 Â· UI & Entry
- `selfai/ui/terminal_ui.py`
- `selfai/selfai.py` (Entry Point)

### Externe Systeme (kritisch)
- AnythingLLM ( erreichbar, Workspace vorhanden, API Key gÃ¼ltig)
- Ollama Modelle (Planner-Modelle wie `qwen3:4b-thinking-2507-q4_K_M`, Merge-Modelle optional)
- Dateisystem Schreibrechte (Memory/Plans)

### Startup-AbhÃ¤ngigkeiten (Graphisch)
start selfai.py  
â†’ `.env` laden  
â†’ `config.yaml` â†’ `AppConfig`  
â†’ init Planner/Merge Provider & Healthchecks  
â†’ init MemorySystem (Ordner anlegen)  
â†’ init AgentManager (YAML laden)  
â†’ init LLM Backend (AnythingLLM > QNN > CPU)  
â†’ UI starten  
â†’ Event Loop (`/plan`, `/switch`, freier Chat)

### Request-Handling Kette
`User Message`  
â†’ `agent_manager.active_agent`  
â†’ `memory_system.load_relevant_context`  
â†’ `llm_interface.stream_generate_response` (AnythingLLM)  
â†’ `TerminalUI` streamt  
â†’ `memory_system.save_conversation`  
â†’ (Optional) `planner_ollama_interface.plan` + `execution_dispatcher` + `_execute_merge_phase`

---

**TEIL 3 Â· Fortschritt Tracking**

### Projektstatus (aktualisiert)

| Phase | Feature/Komponente | Status | Fortschritt | Blocker | Verantwortlich |
|-------|--------------------|--------|-------------|---------|-----------------|
| Foundation | Struktur & Konfig (`config_loader`, Agents, Memory dirs) | âœ… | 100% | â€” | Team |
| Foundation | AnythingLLM Interface (`core/anythingllm_interface.py`) | âœ… | 100% | â€” | Team |
| Foundation | Terminal UI (`ui/terminal_ui.py`) | âœ… | 100% | â€” | Team |
| Planner Phase | Planner Integration (Ollama) | âœ… | 100% | â€” | Team |
| Planner Phase | Plan Validator & Warnings | âœ… | 100% | â€” | Team |
| Execution Phase | Execution Dispatcher (Subtasks) | âœ… | 100% | â€” | Team |
| Execution Phase | Merge Handling & Provider Auswahl | âœ… | 100% | â€” | Team |
| Memory Phase | Conversation Storage | âœ… | 100% | â€” | Team |
| Memory Phase | Kontext-Laden (heuristisch) | âœ… | 100% | â€” | Team |
| Memory Phase | Auto-Kategorisierung | â¸ | 0% | LLM-Design noch offen | TBD |
| Monitoring | Performance Metrics & Telemetrie | ğŸ“‹ | 0% | Instrumentierung fehlt | TBD |
| UX Enhancements | Fehlermeldungen/Tooltips | ğŸ”„ | 70% | Merge/Planner Feedback vorhanden, weitere Verfeinerung mÃ¶glich | Team |
| Testing | Automatisierte Tests | ğŸ“‹ | 0% | Teststrategie noch nicht definiert | TBD |
| Documentation | Laufende Doku (dieses Dokument) | âœ… | laufend | â€” | Code Agent |

GesamtschÃ¤tzung: ~75â€¯% KernfunktionalitÃ¤t, Rest: Auto-Kategorisierung, Telemetrie, Tests.

### Komponenten-Status (Detail)

1. **AnythingLLM Interface**  
   Status: Produktiv. Streaming & Non-Streaming implementiert, Fehler propagiert.

2. **AgentManager**  
   Status: Produktiv. Laden/Switch funktional, UI Integration vorhanden.

3. **MemorySystem**  
   Status: Produktiv fÃ¼r Speichern/Laden. TODO Auto-Kategorisierung & Indizes.

4. **Planner & Validator**  
   Status: Produktiv. JSON-Schema, Clarification-Regeln, Warnmeldungen.

5. **Execution Dispatcher**  
   Status: Produktiv. Streaming, Retries, Statuspersistenz.

6. **Merge Pipeline**  
   Status: Produktiv. Backend-Selektion, Streaming, Metadaten persistiert. Erweiterbar fÃ¼r weitere LLMs.

### Offene TODOs / Technical Debt

- Auto-Kategorisierung der Memories (LLM-Aufruf + Index)
- Kontext-Ranking (statt heuristisch letzte EintrÃ¤ge)
- Performance Telemetrie (Messpunkte, Logging)
- Einheitliche Retry/Backoff-Strategie (Planner/Execution/Merge)
- Integrationstests (Planflow, Merge-Backends, Fehlerpfade)
- Model-agnostische Konfiguration (Definition pro Subtask-Engine)
- Dokumentation fÃ¼r Agent YAML Schema (z.â€¯B. README Abschnitt)

### Broken Chains / Risiken

- Wenn Planner JSON Struktur Ã¤ndert â†’ Validator muss aktualisiert werden
- Windows ARM Wheel-Builds (numpy, llama-cpp-python) benÃ¶tigen VS Build Tools
- Timeout-Konfiguration pro Provider wichtig (Streaming kann >180â€¯s dauern)
- Merge-Backends ohne Streaming-FÃ¤higkeit â†’ Fallback muss erfolgreich sein
- AnythingLLM AusfÃ¤lle: Aktuell kein automatischer Provider-Fallback (z.â€¯B. CPU-Modell fÃ¼r Subtasks)

### Session Template (fÃ¼r zukÃ¼nftige Arbeit)
```
## SESSION: 2025-10-30 â€“ [Feature]

### ğŸ¯ Goal
- â€¦

### âœ… Ergebnisse
- â€¦

### ğŸ”— Ablauf
1. â€¦
2. â€¦

### ğŸ“¦ Dependencies
- â€¦

### ğŸš¨ Issues
- â€¦

### ğŸ“Š Tests
- â€¦

### ğŸ“‹ TODO
- [ ] â€¦
```

---

**Zusammenfassung / NÃ¤chste Schritte**
- Auto-Kategorisierung & Kontext-Ranking entwerfen (AbhÃ¤ngigkeit: verfÃ¼gbares LLM oder Ruleset)
- Telemetrie/Logging Punkte festlegen (Planner Latenz, Merge Dauer, Memory IO)
- Teststrategie definieren (Mock AnythingLLM/Ollama, CLI Command Tests)
- Dokumentation nach jeder Session aktualisieren (Status-Tabellen, offene Blocker)

Dieses Dokument bildet die aktuelle Wissensbasis. Bitte nach jeder Implementierungsrunde anpassen, damit Fortschritt, AbhÃ¤ngigkeiten und offene Punkte immer sichtbar bleiben.
