# üéØ SelfAI Prompt Engineering Guide

## Overview

SelfAI nutzt **4 Hauptprompts** in der DPPM-Pipeline (Decompose ‚Üí Parallel ‚Üí Plan ‚Üí Merge). Dieser Guide zeigt alle Prompts, erkl√§rt ihre Rolle und gibt Best Practices f√ºr wissenschaftliches Prompt Engineering.

---

## üìã Die 4 Hauptprompts

### 1. **PLANNER Prompt** (Plan-Generierung)

**Location**: `selfai/core/planner_minimax_interface.py` (Zeile 89-183)

**Purpose**: Zerlegt User-Goal in unabh√§ngige Subtasks (DPPM-Format)

**Key Sections**:

```
Du agierst als DPPM-Planer (Decompose‚ÄìParallel Plan‚ÄìMerge) f√ºr SelfAI
und erzeugst ausschlie√ülich JSON in folgendem Schema:
{
  "subtasks": [...],
  "merge": {...}
}

DPPM-Vorgaben:
1. Decompose ‚Äì zerlege in unabh√§ngige Subtasks (2-8, bei komplexen auch mehr)
2. Parallel Plan ‚Äì parallel_group f√ºr gleichzeitige Ausf√ºhrung
3. Merge ‚Äì Zusammenf√ºhrungs-Strategie definieren

PARALLELISIERUNG (WICHTIG):
- Gleiche parallel_group = gleichzeitige Ausf√ºhrung (2-3x schneller!)
- depends_on NUR f√ºr echte Abh√§ngigkeiten

KRITISCHE REGEL:
- Liefere finale JSON ausschlie√ülich in Antwort-Ausgabe
  (NICHT im Thinking-Bereich!)
- Kein Markdown, keine Backticks, keine Erkl√§rungen
```

**Output**: DPPM Plan JSON

**Common Issues**:
- ‚ùå Planner gibt JSON in `<think>` Tags ‚Üí L√∂sung: "nicht im Thinking-Bereich" explizit
- ‚ùå Zu wenig Parallelisierung ‚Üí L√∂sung: "OPTIMIERE f√ºr Parallelit√§t" hervorheben

---

### 2. **SUBTASK Prompt** (Einzeltask-Ausf√ºhrung)

**Location**: `selfai/core/execution_dispatcher.py` (nutzt Agent System Prompts)

**Purpose**: F√ºhrt einzelnen Subtask aus

**Structure**:
```
System Prompt: {agent.system_prompt}  # z.B. "Du bist ein Code-Helfer..."
User Prompt: {subtask.objective}       # z.B. "Analysiere Datei X..."

History: {relevant_memory}             # 2-3 relevante fr√ºhere Interaktionen
```

**Output**: Subtask-Ergebnis (Text/Code/Analyse)

**Common Issues**:
- ‚ùå Agent ignoriert Kontext ‚Üí L√∂sung: Objective klarer formulieren
- ‚ùå `[TOOL_CALL]` Artefakte im Output ‚Üí L√∂sung: Post-Processing Filter

---

### 3. **MERGER Prompt** (Ergebnis-Synthese)

**Location**: `selfai/selfai.py` (Zeile 479-507)

**Purpose**: Kombiniert alle Subtask-Ergebnisse zu koh√§renter Antwort

**Full Prompt**:

```
Du bist ein Experte f√ºr Ergebnis-Synthese im DPPM-System.

URSPR√úNGLICHES ZIEL (User-Frage):
{original_goal}

AUSGEF√úHRTE SUBTASKS:
{combined_subtask_outputs}

DEINE AUFGABE:
Beantworte die URSPR√úNGLICHE USER-FRAGE direkt und vollst√§ndig.
Synthetisiere die Subtask-Ergebnisse zu einer koh√§renten Gesamt-Antwort.

KRITISCHE ANFORDERUNGEN:
1. FOKUS: Beantworte NUR die urspr√ºngliche User-Frage (keine Meta-Diskussion!)
2. DIREKTHEIT: Beginne sofort (kein "Ich werde jetzt...", kein "Lass mich...")
3. SYNTHESE: Kombiniere intelligent (nicht copy-paste)
4. REDUNDANZ: Wiederholungen NUR EINMAL erw√§hnen
5. WIDERSPR√úCHE: Identifiziere und l√∂se Widerspr√ºche
6. STRUKTUR: Klare Gliederung mit √úberschriften
7. VOLLST√ÑNDIGKEIT: Alle relevanten Infos einbeziehen
8. PR√ÑGNANZ: So kurz wie m√∂glich, so ausf√ºhrlich wie n√∂tig

AUSGABE-FORMAT:
- KEINE <think> Tags oder interne √úberlegungen!
- KEINE Meta-Kommentare √ºber den Merge-Prozess!
- Beginne direkt mit der Antwort (optional: kurze Executive Summary)
- Verwende Markdown (## f√ºr √úberschriften, - f√ºr Listen)
- Bei Code: Integrierten, lauff√§higen Code (nicht separate Snippets)

MERGE-STRATEGIE (vom Planner):
{strategy}

VORGESCHLAGENE SCHRITTE:
{merge_steps}

ERSTELLE JETZT DIE FINALE ANTWORT:
```

**Output**: Finale Gesamt-Antwort

**Common Issues** (BEHOBEN):
- ‚úÖ `<think>` Tags im Output ‚Üí **L√∂sung**: Explizites Verbot + Regex-Filter
- ‚úÖ Meta-Diskussion statt Antwort ‚Üí **L√∂sung**: "FOKUS: Beantworte NUR die User-Frage"
- ‚úÖ "Ich werde jetzt..." ‚Üí **L√∂sung**: "DIREKTHEIT: Beginne sofort"

**Post-Processing**:
```python
# Remove <think> tags automatically (Zeile 588-594)
import re
merge_response = re.sub(r'<think>.*?</think>', '', merge_response, flags=re.DOTALL).strip()
```

---

### 4. **GEMINI JUDGE Prompt** (Qualit√§ts-Bewertung)

**Location**: `selfai/core/gemini_judge.py` (Zeile 176-228)

**Purpose**: Bewertet komplette Ausf√ºhrung (nach Merge)

**Full Prompt**:

```
Du bist ein unabh√§ngiger Evaluator f√ºr AI-Agenten.
Bewerte die folgende Task-Ausf√ºhrung objektiv und kritisch.

**ORIGINAL GOAL:**
{goal}

**EXECUTION OUTPUT:**
{subtask_outputs + merge_result}

**PLAN DETAILS:**
- Subtasks: {count}
- Engines used: {engines}

**EXECUTION TIME:** {time} seconds

**FILES MODIFIED:** {files}

**BEWERTUNGSAUFGABE:**

Bewerte auf Skala 0-10:
1. Task Completion: Hat es das Ziel erreicht? (0=gar nicht, 10=perfekt)
2. Code Quality: Ist der Code/Output gut? (0=schlecht, 10=exzellent)
3. Efficiency: War die Ausf√ºhrung effizient? (0=verschwenderisch, 10=optimal)
4. Goal Adherence: Passt es genau zum Ziel? (0=verfehlt, 10=genau getroffen)

Antworte NUR mit folgendem JSON (kein anderer Text!):

{
  "task_completion": 8.5,
  "code_quality": 7.0,
  "efficiency": 9.0,
  "goal_adherence": 8.0,
  "summary": "Kurze Zusammenfassung (1-2 S√§tze)",
  "strengths": ["St√§rke 1", "St√§rke 2", "St√§rke 3"],
  "weaknesses": ["Schw√§che 1", "Schw√§che 2"],
  "recommendations": ["Empfehlung 1", "Empfehlung 2"]
}

Sei objektiv, kritisch aber fair. Fokus auf messbare Kriterien.
```

**Output**: JSON Score (üü¢üü°üî¥ Ampel-System)

**Common Issues**:
- ‚úÖ Gemini gibt Help-Output ‚Üí **L√∂sung**: One-Shot Mode (`gemini` ohne Flags)
- ‚úÖ Session-Pollution ‚Üí **L√∂sung**: Kein `--resume`, `stderr=DEVNULL`

---

## üî¨ Wissenschaftliches Prompt Engineering

### Methodik f√ºr Prompt-Optimierung

**1. Problem Identification**
```
Symptom ‚Üí Root Cause ‚Üí Hypothesis ‚Üí Test ‚Üí Measure
```

**Beispiel**: Merger zeigt `<think>` Tags
- Symptom: `<think>Alright, let's get this done...</think>` im Output
- Root Cause: Kein explizites Verbot von Chain-of-Thought Tags
- Hypothesis: Explizites Verbot + Regex-Filter hilft
- Test: Prompt anpassen + Filter implementieren
- Measure: A/B Test mit 10 Tasks, manuelles Review

---

**2. Prompt-Komponenten (nach Li et al. 2023)**

Jeder Prompt sollte haben:

1. **Role Definition**: "Du bist ein Experte f√ºr..."
2. **Task Description**: "Deine Aufgabe ist..."
3. **Input Context**: Klar strukturierte Eingaben
4. **Output Format**: Explizite Format-Anforderungen
5. **Constraints**: Was NICHT tun (kritisch f√ºr LLMs!)
6. **Examples** (optional): Few-shot learning

**SelfAI Merger Prompt** (annotiert):

```
Du bist ein Experte f√ºr Ergebnis-Synthese...     ‚Üê Role Definition

URSPR√úNGLICHES ZIEL (User-Frage):                ‚Üê Input Context
{original_goal}

DEINE AUFGABE:                                   ‚Üê Task Description
Beantworte die URSPR√úNGLICHE USER-FRAGE...

KRITISCHE ANFORDERUNGEN:                         ‚Üê Constraints (neu!)
1. FOKUS: Beantworte NUR die User-Frage
2. KEINE <think> Tags!                           ‚Üê Explicit Constraint

AUSGABE-FORMAT:                                  ‚Üê Output Format
- Markdown-Formatierung
- Keine Meta-Kommentare
```

---

**3. Constraint-First Prompting (Best Practice)**

**Problem**: LLMs tun oft zu viel (Meta-Kommentare, Erkl√§rungen, etc.)

**L√∂sung**: Constraints VOR Aufgabenbeschreibung!

**Vorher** (schlechter Prompt):
```
Synthetisiere die Ergebnisse zu einer Antwort.
Beantworte die User-Frage.
```

**Nachher** (besserer Prompt):
```
KRITISCHE ANFORDERUNGEN:
- KEINE <think> Tags!
- KEINE Meta-Kommentare!
- Beginne direkt mit der Antwort!

DANN: Synthetisiere die Ergebnisse...
```

---

**4. Negative Instructions (Wichtig f√ºr MiniMax/DeepSeek)**

MiniMax Modelle nutzen `<think>` f√ºr Chain-of-Thought. **Du musst das explizit verbieten!**

**Effektive Negative Instructions**:
```
AUSGABE-FORMAT:
- KEINE <think> Tags oder interne √úberlegungen!        ‚Üê Explizit
- KEINE Meta-Kommentare √ºber den Merge-Prozess!       ‚Üê Spezifisch
- Beginne direkt mit der Antwort                      ‚Üê Positiv formuliert
```

**Ineffektive Version**:
```
Gib eine gute Antwort.  ‚Üê Zu vage, keine Constraints
```

---

**5. Testing & Iteration**

**A/B Testing Setup**:

```python
# Test-Cases f√ºr Merger-Prompt
test_cases = [
    {
        "goal": "Erkl√§re Python Decorators",
        "subtasks": [...],
        "expected_keywords": ["@decorator", "wrapper", "function"],
        "forbidden_patterns": [r"<think>", r"Ich werde jetzt", r"Lass mich"]
    },
    # ... 10 weitere Cases
]

# Metrics
def evaluate_merge_output(output, test_case):
    score = 0
    # 1. Enth√§lt erwartete Keywords?
    for kw in test_case["expected_keywords"]:
        if kw.lower() in output.lower():
            score += 10

    # 2. Keine verbotenen Patterns?
    for pattern in test_case["forbidden_patterns"]:
        if re.search(pattern, output):
            score -= 20  # Heavy penalty!

    # 3. Direktheit (beginnt nicht mit Meta-Text)
    if not re.match(r'^(Ich|Lass|Alright|Okay)', output):
        score += 20

    return score
```

---

**6. Prompt Versioning**

Tracking von Prompt-√Ñnderungen:

```
# PROMPT_CHANGELOG.md
## Merger Prompt v2.1 (2025-12-18)
- Added: "KEINE <think> Tags!" constraint
- Added: "DIREKTHEIT" requirement
- Changed: "URSPR√úNGLICHES ZIEL" ‚Üí "URSPR√úNGLICHES ZIEL (User-Frage)"
- Impact: A/B Test zeigt 85% weniger <think> Tags, 60% direktere Antworten

## Merger Prompt v2.0 (2025-12-17)
- Initial structured format
```

---

## üß™ Experimentelle Best Practices

### Chain-of-Thought Kontrolle

**Problem**: MiniMax nutzt `<think>` f√ºr bessere Reasoning, aber Output wird verschmutzt.

**L√∂sungen**:

1. **Explizites Verbot** (current approach):
   ```
   - KEINE <think> Tags in der Ausgabe!
   ```

2. **Separater Thinking-Bereich** (advanced):
   ```
   Du darfst intern nachdenken, aber gib NUR die finale Antwort aus.

   Format:
   [THINKING: deine internen √úberlegungen]
   [ANSWER: finale Antwort f√ºr User]
   ```
   Then filter `[THINKING:...]` in post-processing.

3. **Regex Filter** (failsafe):
   ```python
   output = re.sub(r'<think>.*?</think>', '', output, flags=re.DOTALL)
   ```

---

### Few-Shot Learning (Future Enhancement)

**Currently**: Zero-shot prompts

**Enhancement**: Add examples to critical prompts

```
BEISPIEL-SYNTHESE:

User-Frage: "Wie funktioniert Merge Sort?"

Subtask 1: "Merge Sort ist ein Divide-and-Conquer Algorithmus..."
Subtask 2: "Die Zeitkomplexit√§t ist O(n log n)..."

GUTE Merge-Antwort:
## Merge Sort

Merge Sort ist ein Divide-and-Conquer Algorithmus mit O(n log n)
Zeitkomplexit√§t. Er funktioniert wie folgt:
1. Teile Array in zwei H√§lften
2. Sortiere rekursiv
3. Merge die sortierten H√§lften

SCHLECHTE Merge-Antwort:
<think>Okay, ich muss jetzt die Subtasks kombinieren...</think>
Ich werde jetzt erkl√§ren, wie Merge Sort funktioniert...
```

---

### Temperature & Sampling Control

**Planner**: `temperature=0.1` (deterministisch, strukturiertes JSON)

**Subtasks**: `temperature=0.3` (etwas kreativ, aber fokussiert)

**Merger**: `temperature=0.2` (koh√§rent, aber nicht roboterhaft)

**Judge**: `temperature=0.1` (objektiv, reproduzierbar)

---

## üìä Metrics & Evaluation

### Prompt Quality Metrics

**1. Compliance Rate**: Befolgt der Output die Constraints?
```python
compliance = (outputs_without_think_tags / total_outputs) * 100
```

**2. Directness Score**: Wie direkt beantwortet es die Frage?
```python
# Beginnt mit Meta-Text?
is_direct = not re.match(r'^(Ich|Lass|Alright|Okay)', output)
```

**3. Redundancy Score**: Wie viel Wiederholung?
```python
from difflib import SequenceMatcher
similarity = SequenceMatcher(None, subtask1_output, merge_output).ratio()
# Should be LOW (merger should synthesize, not copy)
```

**4. Goal Adherence**: Beantwortet es die User-Frage?
```python
# Keywords from goal present in output?
goal_keywords = extract_keywords(user_goal)
present_keywords = [kw for kw in goal_keywords if kw in output.lower()]
adherence = len(present_keywords) / len(goal_keywords)
```

---

### Logging f√ºr Wissenschaftliche Analyse

**Aktiviere Prompt Logging**:

```python
# In selfai.py (vor jedem LLM-Call)
def log_prompt(stage, prompt, output, metadata):
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "stage": stage,  # "planner", "subtask", "merger", "judge"
        "prompt": prompt,
        "output": output,
        "metadata": metadata,
        "model": metadata.get("model"),
        "temperature": metadata.get("temperature"),
    }

    with open("memory/prompt_logs.jsonl", "a") as f:
        f.write(json.dumps(log_entry) + "\n")
```

**Analyse mit Pandas**:

```python
import pandas as pd

logs = pd.read_json("memory/prompt_logs.jsonl", lines=True)

# Merger-Performance √ºber Zeit
merger_logs = logs[logs["stage"] == "merger"]
merger_logs["has_think_tags"] = merger_logs["output"].str.contains("<think>")

print(f"Think Tag Rate: {merger_logs['has_think_tags'].mean() * 100:.1f}%")

# A/B Test: Prompt v2.0 vs v2.1
v2_0 = merger_logs[merger_logs["metadata"].str.contains("v2.0")]
v2_1 = merger_logs[merger_logs["metadata"].str.contains("v2.1")]

print(f"v2.0 Think Rate: {v2_0['has_think_tags'].mean() * 100:.1f}%")
print(f"v2.1 Think Rate: {v2_1['has_think_tags'].mean() * 100:.1f}%")
```

---

## üéì Advanced Techniques

### 1. **Instruction Hierarchy** (Li et al. 2023)

```
KRITISCH (must-have):
- KEINE <think> Tags!

WICHTIG (should-have):
- Markdown-Formatierung

OPTIONAL (nice-to-have):
- Code-Beispiele
```

### 2. **Constraint Cascading**

```
GLOBAL CONSTRAINTS (f√ºr alle Outputs):
- Max 2000 Zeichen
- Markdown-Format

STAGE-SPECIFIC CONSTRAINTS:
Merger:
  - KEINE Meta-Kommentare
  - Direkte Antwort
```

### 3. **Prompt Chaining** (schon implementiert!)

```
User Goal
  ‚Üí Planner Prompt ‚Üí DPPM Plan
    ‚Üí Subtask Prompts ‚Üí Ergebnisse
      ‚Üí Merger Prompt ‚Üí Finale Antwort
        ‚Üí Judge Prompt ‚Üí Bewertung
```

---

## üìö References

- **Li et al. (2023)**: "Guiding Large Language Models via Directional Stimulus Prompting"
- **Wei et al. (2022)**: "Chain-of-Thought Prompting Elicits Reasoning in LLMs"
- **OpenAI (2024)**: "Prompt Engineering Guide"
- **Anthropic (2024)**: "Claude Prompt Engineering Best Practices"

---

## üîß Tools for Prompt Engineering

### 1. Prompt Testing Framework

Create `selfai/tools/prompt_tester.py`:

```python
def test_prompt(prompt_template, test_cases, model_interface):
    """A/B test prompt variations"""
    results = []
    for case in test_cases:
        prompt = prompt_template.format(**case["inputs"])
        output = model_interface.generate(prompt)

        score = evaluate(output, case["expected"])
        results.append({
            "case": case["name"],
            "score": score,
            "output": output
        })

    return pd.DataFrame(results)
```

### 2. Prompt Diff Tool

```bash
# Compare prompts
diff -u <(grep -A50 "final_prompt =" selfai/selfai.py.v2.0) \
        <(grep -A50 "final_prompt =" selfai/selfai.py.v2.1)
```

### 3. Output Quality Checker

```python
def check_merger_quality(output):
    checks = {
        "has_think_tags": bool(re.search(r'<think>', output)),
        "has_meta_commentary": bool(re.match(r'^(Ich|Lass|Alright)', output)),
        "has_markdown": bool(re.search(r'##', output)),
        "word_count": len(output.split()),
        "is_direct": not re.match(r'^(Ich werde|Lass mich)', output)
    }
    return checks
```

---

## üöÄ Next Steps

### Immediate Improvements

1. ‚úÖ **Merger Prompt v2.1** (DONE)
   - Added: KEINE `<think>` Tags
   - Added: DIREKTHEIT requirement
   - Added: Regex filter

2. üîÑ **Test with real tasks**
   - Run 10 /plan executions
   - Measure: `<think>` tag rate, directness, goal adherence

3. üìä **Enable Prompt Logging**
   - Log all prompts + outputs
   - Track metrics over time

### Future Enhancements

1. **Few-Shot Examples** in Merger
2. **Dynamic Temperature** based on task complexity
3. **Prompt Versioning System** with A/B testing
4. **Automated Prompt Optimization** (genetic algorithms?)
5. **Multi-Model Ensemble** (Gemini + MiniMax for merge)

---

**Last Updated**: 2025-12-18
**Version**: 1.0
**Status**: ‚úÖ Active Development
