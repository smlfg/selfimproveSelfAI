# SelfAI Hybrid Inference Configuration Template
# Copy this file to config.yaml and adjust the values as needed.

# --- NPU Provider (AnythingLLM) ---
# Used for the primary NPU-accelerated backend.
# The API_KEY is loaded securely from your .env file.
npu_provider:
  base_url: "http://localhost:3001/api/v1" # Local endpoint of the AnythingLLM server
  workspace_slug: "main" # The workspace to use in AnythingLLM

# --- CPU Fallback (llama-cpp-python) ---
# Used when the NPU backend is unavailable.
cpu_fallback:
  # Path to the local GGUF model file inside the 'models' directory.
  model_path: "Phi-3-mini-4k-instruct.Q4_K_M.gguf"
  n_ctx: 4096 # Maximum token context length
  n_gpu_layers: 0 # Number of layers to offload to GPU. 0 for pure CPU.

# --- System Settings ---
system:
  streaming_enabled: true # Enable word-by-word streaming output

# --- Agent Configuration ---
agent_config:
  default_agent: "code_helfer" # The default agent to load on startup

# --- Planner (Ollama, optional) ---
planner:
  enabled: false # Enable Ollama-based planning layer
  execution_timeout: 120.0 # Seconds allowed per Subtask LLM execution
  providers:
    - name: ollama-cloud
      type: remote_ollama # or local_ollama
      base_url: "https://api.ollama.ai"
      model: "qwen2.5:3b-instruct"
      timeout: 120.0
      max_tokens: 1024
      headers:
        Authorization: "Bearer ${OLLAMA_CLOUD_API_KEY}"
    - name: local-ollama
      type: local_ollama
      base_url: "http://localhost:11434"
      model: "gemma3:1b"
      timeout: 180.0
      max_tokens: 768
