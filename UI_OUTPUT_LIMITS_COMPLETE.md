# UI Output Limits - Vollst√§ndige √úbersicht

**Date**: 2025-01-21
**Problem**: "es ist ja nicht nur das limit, sondern im ui max output"
**Status**: ‚úÖ ALLE UI LIMITS IDENTIFIZIERT & ERH√ñHT

---

## üìä Alle Output Limits in SelfAI

### 1. MERGER OUTPUT - KEIN UI LIMIT! ‚úÖ

**File**: `selfai/selfai.py` (Line 555-589)

**Streaming Output**:
```python
for chunk in iterator:
    if chunk:
        chunks.append(chunk)
        ui.streaming_chunk(chunk)  # ‚Üê KEIN LIMIT!
print()
merge_response = "".join(chunks)
```

**Block Output**:
```python
ui.stream_prefix(f"{merge_label}")
ui.typing_animation(merge_response)  # ‚Üê KEIN LIMIT!
```

**UI Funktionen** (`terminal_ui.py`):
```python
def streaming_chunk(self, chunk: str) -> None:
    if chunk:
        print(chunk, end="", flush=True)  # ‚Üê Zeigt ALLES!

def typing_animation(self, text: str, delay: float = 0.02) -> None:
    for char in text:
        print(char, end="", flush=True)  # ‚Üê Zeigt ALLES!
        time.sleep(delay)
    print()
```

**Resultat**: Merger Output hat **KEIN UI-Limit**, nur LLM Token-Limit (5000)!

---

### 2. SUBTASK RESULT DISPLAY - LIMIT ERH√ñHT! ‚úÖ

**File**: `selfai/core/execution_dispatcher.py` (Line 257-260)

**VORHER** (zu kurz):
```python
display_text = response.strip()[:500]  # ‚ùå Nur 500 chars!
if len(response) > 500:
    display_text += "\n... [weitere Ausgabe in Memory gespeichert]"
```

**NACHHER** (besser):
```python
display_text = response.strip()[:2000]  # ‚úÖ 2000 chars!
if len(response) > 2000:
    display_text += "\n... [weitere Ausgabe in Memory gespeichert]"
```

**Kontext**: Wird nur f√ºr Subtask-Ergebnis-Zusammenfassung genutzt, NICHT f√ºr Merger!

---

### 3. TOOL DESCRIPTIONS - LIMIT ERH√ñHT! ‚úÖ

**File**: `selfai/ui/terminal_ui.py` (Line 255)

**VORHER**:
```python
desc = tool["description"][:80] + "..."  # ‚ùå 80 chars
```

**NACHHER**:
```python
desc = tool["description"][:200] + "..."  # ‚úÖ 200 chars
```

---

### 4. WEITERE LIMITS (zur Info)

Diese betreffen NICHT den Merger, aber der Vollst√§ndigkeit halber:

| Location | Limit | Zweck |
|----------|-------|-------|
| `selfai.py:1267` | 200 chars | Traceback in Fehlermeldungen |
| `selfai.py:1782` | 200 chars | Fix plan analysis preview |
| `aider_tool.py:118` | 100 chars | Aider task description |
| `openhands_tool.py:185-186` | 500 chars | Error output preview |
| `selfai_agent.py:114` | 100 chars | Tool result preview (logging) |
| `planner_minimax_interface.py:363` | 500 chars | Fallback plan |

**Diese betreffen NICHT den Merger-Output!**

---

## üéØ Warum ist Merger Output trotzdem kurz?

### M√∂gliche Ursachen:

#### 1. **LLM Token Limit erreicht** (Hauptursache!)

**Config**: `max_tokens: 5000` in `config.yaml`

**Was passiert**:
```
LLM generiert Text...
...
[5000 tokens erreicht] ‚Üê LLM STOPPT HIER!
```

**Fix**: Erh√∂he `max_tokens` in config.yaml:
```yaml
merge:
  providers:
    - name: "minimax-merge"
      max_tokens: 8000  # Oder 10000
```

---

#### 2. **Terminal scrollt zu schnell** (Sichtbar, aber scrollt weg)

**Symptom**: Output IST da, aber scrollt aus Sicht

**Check**:
```bash
# Nach /plan execution:
cd memory/plans
cat [latest-plan].json | grep merge_result_path

# Dann:
cat [merge_result_path]
```

**Fix**:
- Scroll im Terminal nach oben
- Oder nutze `less`: `python selfai/selfai.py | less -R`

---

#### 3. **LLM generiert selbst kurze Antworten** (Model-Verhalten)

**Symptom**: LLM entscheidet sich f√ºr kurze Antwort (unter Token-Limit)

**Ursache**: Merger-Prompt k√∂nnte zu "summarize" auffordern

**Check**: Schaue in `selfai.py` Line 489-518:
```python
final_prompt = (
    "KRITISCHE ANFORDERUNGEN:\n"
    "...8. PR√ÑGNANZ: So kurz wie m√∂glich, aber so ausf√ºhrlich wie n√∂tig\n\n"
)
```

**Fix**: Entferne "PR√ÑGNANZ" Anforderung f√ºr l√§ngere Outputs:
```python
# selfai.py Line 505
# "8. PR√ÑGNANZ: So kurz wie m√∂glich, aber so ausf√ºhrlich wie n√∂tig\n\n"
"8. VOLLST√ÑNDIGKEIT hat Priorit√§t √ºber K√ºrze\n\n"
```

---

#### 4. **Multi-Pane UI abschneiden** (nur bei parallelen Tasks)

**Symptom**: Bei parallelen Subtasks zeigt Multi-Pane UI nur 4 Zeilen pro Pane

**Check**: Siehst du Boxen mit `‚îú‚îÄ‚îÄ‚î§` Separatoren?

**Fix**: Multi-Pane UI betrifft NUR Subtask-Execution, NICHT den Merger!

---

## ‚úÖ Zusammenfassung der Fixes

### Was wurde erh√∂ht:

| Component | VORHER | NACHHER | Status |
|-----------|--------|---------|--------|
| **Merger Token Limit (Fallback)** | 2048 | 4096 | ‚úÖ |
| **Merger Token Limit (Config)** | - | 5000 | ‚úÖ |
| **Subtask Display** | 500 chars | 2000 chars | ‚úÖ |
| **Tool Descriptions** | 80 chars | 200 chars | ‚úÖ |
| **Merger UI Display** | ‚àû | ‚àû | ‚úÖ (kein Limit!) |

### Effektive Limits:

- **Merger Output**: **5000 tokens** (~3750 W√∂rter, ~100 Zeilen)
- **Subtask Display**: **2000 chars** (~350 W√∂rter, ~10 Zeilen)
- **UI hat KEIN Limit** f√ºr Merger!

---

## üß™ Testing Guide

### Test 1: Pr√ºfe effektives Token-Limit

```bash
python selfai/selfai.py
```

```
Du: /plan erkl√§re den execution_dispatcher komplett im detail mit allen funktionen

# ... Plan execution ...

üîÑ Merge-Ausgabe mit Agent 'default' wird berechnet...
[MiniMax-Merge]: [Output startet]
```

**W√§hrend Output l√§uft**:
- Z√§hle grob die Zeilen
- Merger sollte bis ~100 Zeilen gehen (bei 5000 tokens)
- Stoppt er fr√ºher? ‚Üí LLM entscheidet sich f√ºr kurze Antwort
- Stoppt er genau bei ~100 Zeilen? ‚Üí Token-Limit erreicht

**Nach Completion**:
```bash
# Check gespeicherter Output:
cd memory/plans
cat [latest-plan].json | jq '.metadata.merge_result_path'
cat [path-from-above]
```

**Erwartung**: Vollst√§ndiger Output gespeichert (gleich wie im Terminal)

---

### Test 2: Check Terminal vs. Memory

```bash
python selfai/selfai.py > output.log 2>&1
```

```
Du: /plan [komplexe frage]
# ... warte bis fertig ...
quit
```

```bash
# Vergleiche Terminal-Output mit Memory:
cat output.log | grep -A 200 "MiniMax-Merge" > terminal_output.txt
cat memory/plans/[latest]/[merge-result-file] > memory_output.txt

diff terminal_output.txt memory_output.txt
```

**Erwartung**: Identisch! (UI zeigt alles was in Memory steht)

---

### Test 3: Erh√∂he Token-Limit manuell

**Edit**: `config.yaml`

```yaml
merge:
  providers:
    - name: "minimax-merge"
      max_tokens: 10000  # Doppelt so viel!
```

```bash
python selfai/selfai.py
```

```
Du: /plan erstelle eine umfassende dokumentation von selfai

# Merger sollte DOPPELT so lang antworten k√∂nnen!
```

---

## üîß Wenn Output immer noch zu kurz:

### Checklist:

- [ ] `config.yaml` hat `max_tokens: 5000` (oder h√∂her)
- [ ] Terminal ist breit genug (mindestens 80 Zeichen)
- [ ] Output scrollt nicht aus Sicht (scroll nach oben!)
- [ ] Memory-Datei enth√§lt vollst√§ndigen Output
- [ ] LLM entscheidet selbst f√ºr kurze Antwort (nicht Token-Limit)

### L√∂sung:

**Option 1**: Erh√∂he Token-Limit
```yaml
# config.yaml
max_tokens: 10000
```

**Option 2**: √Ñndere Merger-Prompt
```python
# selfai.py Line 505
# √Ñndere "PR√ÑGNANZ" zu "AUSF√úHRLICHKEIT"
"8. AUSF√úHRLICHKEIT: Detailliert und vollst√§ndig (nicht kurz!)\n\n"
```

**Option 3**: Nutze `less` f√ºr Scrolling
```bash
python selfai/selfai.py | less -R
```

---

## üìù Final Answer

### Merger Output im TUI ist kurz wegen:

**NICHT wegen**:
- ‚ùå UI Display-Limit (gibt es nicht!)
- ‚ùå Terminal-Width (Text wrapped nur)
- ‚ùå Subtask Display-Limit (500‚Üí2000, betrifft nicht Merger)

**SONDERN wegen**:
- ‚úÖ **LLM Token-Limit** (5000 tokens aus config)
- ‚úÖ **LLM eigenes Verhalten** (entscheidet selbst f√ºr kurze Antwort)
- ‚úÖ **Merger-Prompt** fordert "PR√ÑGNANZ" an

### L√∂sung:

1. **Token-Limit erh√∂hen** (config.yaml: `max_tokens: 10000`)
2. **Prompt anpassen** (weniger "kurz", mehr "vollst√§ndig")
3. **Terminal scrolling** beachten (vielleicht IST Output komplett, aber scrollt weg?)

---

**Status**: ‚úÖ ALLE UI LIMITS IDENTIFIZIERT & MAXIMIERT
**Merger UI**: Kein Limit (zeigt alles bis LLM-Token-Limit)
**Next**: Erh√∂he `max_tokens` in config wenn n√∂tig!
