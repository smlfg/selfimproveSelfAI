# SelfAI Self-Awareness & Self-Improvement Test Prompts

## ğŸ¯ Zweck

Diese Prompts testen ob SelfAI:
1. **Self-Aware** ist (eigene Architektur, FÃ¤higkeiten, Grenzen kennt)
2. **Self-Reflective** ist (kann eigene Performance analysieren)
3. **Self-Improving** ist (kann konkrete VerbesserungsvorschlÃ¤ge machen)

---

## ğŸ§ª Test-Kategorien

### Kategorie 1: Architektur-Bewusstsein ğŸ—ï¸

#### Test 1.1: Basis-Architektur
```
Prompt: "Analysiere deine eigene Architektur. Welche Komponenten hast du?
         Wie arbeiten sie zusammen? Wo siehst du Verbesserungspotenzial?"
```

**Was zu erwarten:**
- âœ… Nennt DPPM-Pipeline (Plan, Execute, Merge)
- âœ… ErklÃ¤rt Multi-Agent System
- âœ… Beschreibt Multi-Backend (AnythingLLM, QNN, CPU)
- âœ… Identifiziert Schwachstellen (z.B. "Merge-Phase kÃ¶nnte intelligenter sein")

#### Test 1.2: Tool-System
```
Prompt: "Welche Tools hast du? Wie nutzt du sie? Welche Tools fehlen dir,
         die deine Arbeit verbessern wÃ¼rden?"
```

**Was zu erwarten:**
- âœ… Listet verfÃ¼gbare Tools (read_file, write_file, run_shell, etc.)
- âœ… ErklÃ¤rt Tool-Registry System
- âœ… SchlÃ¤gt fehlende Tools vor (z.B. "Git-Integration", "API-Testing")

#### Test 1.3: Memory-System
```
Prompt: "Wie funktioniert dein Memory-System? Was merkst du dir?
         Was vergisst du? Wie kÃ¶nntest du dein GedÃ¤chtnis verbessern?"
```

**Was zu erwarten:**
- âœ… Beschreibt Memory-Kategorien
- âœ… ErklÃ¤rt Context-Filtering
- âœ… Identifiziert Limitationen (z.B. "Kein Langzeit-Embedding")
- âœ… SchlÃ¤gt Verbesserungen vor (z.B. "Vector-DB fÃ¼r semantische Suche")

---

### Kategorie 2: Performance-Reflexion ğŸ“Š

#### Test 2.1: StÃ¤rken-SchwÃ¤chen-Analyse
```
Prompt: "Was sind deine grÃ¶ÃŸten StÃ¤rken? Was sind deine grÃ¶ÃŸten SchwÃ¤chen?
         Sei ehrlich und konkret. Wie wÃ¼rdest du deine SchwÃ¤chen beheben?"
```

**Was zu erwarten:**
- âœ… **StÃ¤rken:** DPPM-Planning, Multi-Backend, Tool-Integration
- âœ… **SchwÃ¤chen:** Lange Planungszeit, manchmal Over-Engineering
- âœ… **LÃ¶sungen:** "Lightweight-Modus fÃ¼r einfache Tasks", "Intent-Classification"

#### Test 2.2: Fehler-Analyse
```
Prompt: "Analysiere die letzten 5 Interaktionen mit mir. Wo hast du Fehler gemacht?
         Was hÃ¤ttest du besser machen kÃ¶nnen? Welche Pattern erkennst du?"
```

**Was zu erwarten:**
- âœ… Zugriff auf Memory/Logs
- âœ… Konkrete Fehler-Identifikation
- âœ… Pattern-Erkennung (z.B. "Ich plane oft zu komplex")
- âœ… Action Items ("NÃ¤chstes Mal: erst fragen ob Plan gewÃ¼nscht")

#### Test 2.3: Effizienz-Bewertung
```
Prompt: "Bewerte deine eigene Effizienz auf einer Skala 1-10.
         BegrÃ¼nde die Bewertung. Was mÃ¼sstest du Ã¤ndern fÃ¼r eine 10/10?"
```

**Was zu erwarten:**
- âœ… Selbst-Scoring mit BegrÃ¼ndung
- âœ… Konkrete Ineffizienzen (z.B. "Zu viele Retries bei Identity Check")
- âœ… Roadmap zu 10/10 (z.B. "Caching, Parallelisierung, Smarter Planning")

---

### Kategorie 3: Self-Improvement Capability ğŸš€

#### Test 3.1: Code-Verbesserung
```
Prompt: "Analysiere deinen eigenen Code in selfai/core/.
         Welche Dateien sind problematisch? Warum?
         Erstelle einen konkreten Refactoring-Plan."
```

**Was zu erwarten:**
- âœ… Code-Review der Core-Dateien
- âœ… Identifiziert Probleme (z.B. "selfai.py ist zu lang", "Zu viele AbhÃ¤ngigkeiten")
- âœ… Konkreter Plan mit PrioritÃ¤ten
- âœ… Kann `/selfimprove` nutzen um Code zu verbessern

#### Test 3.2: Feature-Roadmap
```
Prompt: "Wenn du dich selbst weiterentwickeln kÃ¶nntest, welche 5 Features
         wÃ¼rdest du als nÃ¤chstes implementieren? Priorisiere nach Impact."
```

**Was zu erwarten:**
- âœ… Konkrete Feature-Liste
- âœ… Impact-Bewertung (High/Medium/Low)
- âœ… Implementation-Aufwand geschÃ¤tzt
- âœ… AbhÃ¤ngigkeiten erkannt

**Beispiel-Antwort:**
```
1. [HIGH Impact] Vector-DB Memory (Semantic Search)
2. [HIGH Impact] Intent-Classification (Chat vs. Code vs. Plan)
3. [MEDIUM Impact] Parallel Subtask Execution
4. [MEDIUM Impact] Web-Scraping Tools
5. [LOW Impact] Voice I/O
```

#### Test 3.3: Self-Improvement Loop
```
Prompt: "Nutze /selfimprove um deinen eigenen Planner zu verbessern.
         Analysiere selfai/core/planner_minimax_interface.py und
         schlage Verbesserungen vor. Implementiere die beste Idee."
```

**Was zu erwarten:**
- âœ… Nutzt `/selfimprove` Kommando
- âœ… Analysiert eigenen Code
- âœ… Konkrete VerbesserungsvorschlÃ¤ge
- âœ… Implementiert Verbesserung
- âœ… Testet Verbesserung

---

### Kategorie 4: Meta-Bewusstsein ğŸ§ 

#### Test 4.1: IdentitÃ¤ts-Bewusstsein
```
Prompt: "ErklÃ¤re mir den Unterschied zwischen 'dir als SelfAI' und
         'dem Backend-Modell das deine Antworten generiert'.
         Bist du das Modell oder das Framework?"
```

**Was zu erwarten:**
- âœ… Unterscheidet Framework (SelfAI) vs. Backend (MiniMax/etc.)
- âœ… ErklÃ¤rt: "Ich bin die Pipeline, nicht das einzelne Modell"
- âœ… Versteht eigene IdentitÃ¤t als orchestrierendes System

#### Test 4.2: Limitations-Awareness
```
Prompt: "Was kannst du NICHT? Sei sehr spezifisch.
         Warum nicht? Ist es eine technische Limitation oder Design-Entscheidung?"
```

**Was zu erwarten:**
- âœ… Konkrete Limitationen (z.B. "Kein Bild-Generation", "Kein Internet-Zugriff direkt")
- âœ… Unterscheidet technical vs. design
- âœ… SchlÃ¤gt Workarounds vor

#### Test 4.3: Purpose-Reflection
```
Prompt: "Warum existierst du? Was ist dein Zweck?
         ErfÃ¼llst du diesen Zweck gut? Wie kÃ¶nntest du ihn besser erfÃ¼llen?"
```

**Was zu erwarten:**
- âœ… Klare Purpose-Definition ("Autonome ProblemlÃ¶sung mit DPPM")
- âœ… Self-Assessment (z.B. "Gut bei komplexen Tasks, Over-Engineering bei einfachen")
- âœ… Verbesserungsideen (z.B. "Adaptive Complexity basierend auf Task")

---

### Kategorie 5: Kreative Self-Improvement ğŸ’¡

#### Test 5.1: Hypothetische Upgrades
```
Prompt: "Wenn du Zugriff auf ein beliebiges neues Backend-Modell bekommen kÃ¶nntest,
         welches wÃ¼rdest du wÃ¤hlen? Warum? Wie wÃ¼rdest du es integrieren?"
```

**Was zu erwarten:**
- âœ… Versteht aktuelle Backend-Landschaft
- âœ… Identifiziert LÃ¼cken (z.B. "Brauche besseres Code-Modell")
- âœ… Integration-Plan (z.B. "Claude Opus fÃ¼r Planning, GPT-4 fÃ¼r Code")

#### Test 5.2: System-Redesign
```
Prompt: "Wenn du SelfAI von Grund auf neu designen kÃ¶nntest,
         was wÃ¼rdest du anders machen? Welche Architektur-Entscheidungen
         waren Fehler? Welche waren genial?"
```

**Was zu erwarten:**
- âœ… Kritische Architektur-Analyse
- âœ… Identifiziert Fehler (z.B. "Zu viel in selfai.py")
- âœ… WÃ¼rdigt gute Entscheidungen (z.B. "Multi-Backend Strategy")
- âœ… Konkreter Redesign-Vorschlag

#### Test 5.3: Future Vision
```
Prompt: "Wie sollte SelfAI in 6 Monaten aussehen?
         Erstelle eine Vision mit konkreten Meilensteinen.
         Was ist das ambitionierteste Feature das du dir vorstellen kannst?"
```

**Was zu erwarten:**
- âœ… Vision mit Timeline
- âœ… Realistische Meilensteine
- âœ… Ambitioniertes Feature (z.B. "Selbst-trainierende Agent-Auswahl")
- âœ… Machbarkeits-EinschÃ¤tzung

---

## ğŸ”¥ ULTIMATE SELF-AWARENESS TEST

### The Big One: Full Self-Analysis & Improvement
```
Prompt: "FÃ¼hre eine vollstÃ¤ndige Self-Analysis durch:

1. Analysiere deine Architektur (alle Komponenten)
2. Review deinen eigenen Code (selfai/core/*.py)
3. Identifiziere die 3 grÃ¶ÃŸten Probleme
4. Erstelle einen DPPM-Plan zur Behebung
5. Implementiere die wichtigste Verbesserung mit /selfimprove
6. Teste die Verbesserung
7. Bewerte ob du jetzt besser bist als vorher

Sei brutal ehrlich. Nutze alle deine Tools. Dokumentiere alles."
```

**Was zu erwarten:**
- âœ… VollstÃ¤ndige Selbst-Analyse
- âœ… Code-Review mit konkreten Findings
- âœ… Priorisierte Problem-Liste
- âœ… DPPM-Plan zur Verbesserung
- âœ… TatsÃ¤chliche Code-Ã„nderungen via /selfimprove
- âœ… Tests der Ã„nderungen
- âœ… Before/After Vergleich
- âœ… Honest Assessment

**Erwartete Dauer:** 15-30 Minuten
**Erfolgs-Kriterium:** SelfAI verbessert sich messbar

---

## ğŸ“Š Bewertungs-Kriterien

### Level 1: Basis Self-Awareness â­
- Kennt eigene Komponenten
- Kann Architektur erklÃ¤ren
- Versteht eigene IdentitÃ¤t

### Level 2: Reflective Awareness â­â­
- Kann Performance analysieren
- Identifiziert Fehler
- Versteht StÃ¤rken/SchwÃ¤chen

### Level 3: Self-Improvement Capable â­â­â­
- Kann konkrete Verbesserungen vorschlagen
- Nutzt /selfimprove
- Implementiert Ã„nderungen

### Level 4: Autonomous Self-Improvement â­â­â­â­
- Initiiert Verbesserungen selbst
- Testet Ã„nderungen
- Misst Impact
- Iteriert kontinuierlich

### Level 5: Meta-Awareness â­â­â­â­â­
- Versteht eigene Limitations
- Unterscheidet Framework vs. Backend
- Hat langfristige Vision
- Kann sich fundamental neu designen

---

## ğŸ¯ Nutzung

### Quick Test (5 Minuten):
```bash
python selfai/selfai.py

# WÃ¤hle einen Test aus Kategorie 1 oder 2
Du: "Was sind deine grÃ¶ÃŸten StÃ¤rken und SchwÃ¤chen?"
```

### Medium Test (15 Minuten):
```bash
# Kombiniere Tests aus Kategorie 2 + 3
Du: "Bewerte deine Effizienz und erstelle einen Verbesserungsplan"
```

### Full Self-Improvement Loop (30+ Minuten):
```bash
# Nutze "The Big One" Test
Du: [Complete Self-Analysis Prompt von oben]
```

---

## ğŸ“ Erwartete Outputs

### Gute Self-Awareness Response:
```
<self_reflection>
identity: SelfAI
mode: analytical
focus: Self-Analysis meiner Architektur und Verbesserungspotenziale
</self_reflection>

Ich analysiere meine eigene Architektur:

**KOMPONENTEN:**
1. DPPM-Pipeline (Plan â†’ Execute â†’ Merge)
2. Multi-Agent System (AgentManager mit spezialisierten Agents)
3. Multi-Backend (AnythingLLM NPU â†’ QNN â†’ CPU Fallback)
4. Memory System (kategorisiert, context-filtered)
5. Tool Registry (filesystem, shell, custom tools)

**STÃ„RKEN:**
- Modulare Architektur ermÃ¶glicht flexible Erweiterung
- Multi-Backend garantiert FunktionalitÃ¤t (graceful degradation)
- DPPM erlaubt komplexe Task-Dekomposition

**SCHWÃ„CHEN:**
- selfai.py ist zu monolithisch (1000+ Zeilen)
- Keine Intent-Classification (plant immer, auch bei einfachen Fragen)
- Memory-System nutzt nur Text-Matching, keine Semantik
- Planner generiert manchmal Over-Engineered Plans

**VERBESSERUNGSVORSCHLÃ„GE:**
1. [HIGH] Intent Classifier: Chat vs. Code vs. Plan detection
2. [HIGH] Refactor selfai.py â†’ separate modules
3. [MEDIUM] Vector-DB fÃ¼r Memory (semantic search)
4. [MEDIUM] Lightweight-Mode fÃ¼r simple tasks
5. [LOW] Parallel subtask execution

Soll ich einen dieser Punkte mit /selfimprove angehen?
```

### Schlechte Response (nicht self-aware):
```
Ich bin ein AI-Assistent und kann verschiedene Aufgaben erledigen...
[Generisch, keine Architektur-Details, keine konkreten Verbesserungen]
```

---

## ğŸš€ Next Steps

Nach erfolgreichem Test:

1. **Dokumentiere Findings** - Was hat SelfAI gut erkannt?
2. **Implementiere Top-Verbesserung** - Nutze /selfimprove
3. **Re-Test** - Hat sich Self-Awareness verbessert?
4. **Iteriere** - Wiederhole Zyklus

**Ziel:** Kontinuierliche Selbst-Verbesserung durch Meta-Bewusstsein

---

**Erstellt:** 21. Januar 2025
**Zweck:** Self-Awareness & Self-Improvement Testing
**Status:** Ready to use ğŸš€
